# æ¨¡å¼è¯†åˆ«ä¸æœºå™¨å­¦ä¹ æŠ¥å‘Š
| å§“å   | å­¦å·       | ç­çº§     |
| ------ | ---------- | -------- |
| å§šç¿”å®‡ | 2021302474 | 10042101 |

</div>  

- [æ¨¡å¼è¯†åˆ«ä¸æœºå™¨å­¦ä¹ æŠ¥å‘Š](#æ¨¡å¼è¯†åˆ«ä¸æœºå™¨å­¦ä¹ æŠ¥å‘Š)
  - [ä»»åŠ¡1ï¼šå¤šé¡¹å¼å›å½’](#ä»»åŠ¡1å¤šé¡¹å¼å›å½’)
    - [1.ä»»åŠ¡æè¿°](#1ä»»åŠ¡æè¿°)
    - [2.æ•°æ®é›†æè¿°](#2æ•°æ®é›†æè¿°)
    - [3.ä»»åŠ¡åˆ†æ](#3ä»»åŠ¡åˆ†æ)
      - [æ™®é€šæœ€å°äºŒä¹˜æ³•](#æ™®é€šæœ€å°äºŒä¹˜æ³•)
    - [4.ä»£ç å®ç°ä¸åˆ†æ](#4ä»£ç å®ç°ä¸åˆ†æ)
      - [4.1 æ•°æ®é›†è¯»å–ä¸æ•°æ®é¢„å¤„ç†](#41-æ•°æ®é›†è¯»å–ä¸æ•°æ®é¢„å¤„ç†)
      - [4.2 æ¨¡å‹è®­ç»ƒ](#42-æ¨¡å‹è®­ç»ƒ)
      - [4.3 æ¨¡å‹è¯„ä»·ä¸é¢„æµ‹](#43-æ¨¡å‹è¯„ä»·ä¸é¢„æµ‹)
      - [4.4 ç»“æœå¯è§†åŒ–](#44-ç»“æœå¯è§†åŒ–)
    - [5.æµ‹è¯•ç»“æœå±•ç¤º](#5æµ‹è¯•ç»“æœå±•ç¤º)
      - [5.1 ä¸åŒé˜¶æ•°ä¸‹çš„MSE](#51-ä¸åŒé˜¶æ•°ä¸‹çš„mse)
      - [5.2 ä¸åŒé˜¶æ•°ä¸‹çš„å›¾åƒæ›²çº¿ä»¥åŠè¡¨è¾¾å¼](#52-ä¸åŒé˜¶æ•°ä¸‹çš„å›¾åƒæ›²çº¿ä»¥åŠè¡¨è¾¾å¼)
  - [ä»»åŠ¡2ï¼šæ¦‚ç‡åˆ†ç±»æ³•](#ä»»åŠ¡2æ¦‚ç‡åˆ†ç±»æ³•)
    - [1.ä»»åŠ¡æè¿°](#1ä»»åŠ¡æè¿°-1)
    - [2.æ•°æ®é›†æè¿°](#2æ•°æ®é›†æè¿°-1)
    - [3.ä»»åŠ¡åˆ†æ](#3ä»»åŠ¡åˆ†æ-1)
      - [æ•°å­¦åŸºç¡€](#æ•°å­¦åŸºç¡€)
        - [è´å¶æ–¯ä¼°è®¡](#è´å¶æ–¯ä¼°è®¡)
        - [æœ€å¤§ä¼¼ç„¶ä¼°è®¡](#æœ€å¤§ä¼¼ç„¶ä¼°è®¡)
    - [4.ä»£ç å®ç°ä¸åˆ†æ](#4ä»£ç å®ç°ä¸åˆ†æ-1)
      - [4.1 æ•°æ®é›†è¯»å–ä¸åˆ’åˆ†](#41-æ•°æ®é›†è¯»å–ä¸åˆ’åˆ†)
      - [4.2 å®šä¹‰é«˜æ–¯æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨ç±»](#42-å®šä¹‰é«˜æ–¯æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨ç±»)
      - [4.3 è®­ç»ƒæ¨¡å‹å¹¶è¯„ä¼°](#43-è®­ç»ƒæ¨¡å‹å¹¶è¯„ä¼°)
      - [4.4 æµ‹è¯•æ¨¡å‹å¹¶ä¿å­˜ç»“æœ](#44-æµ‹è¯•æ¨¡å‹å¹¶ä¿å­˜ç»“æœ)
    - [5.è¿è¡Œç»“æœå±•ç¤º](#5è¿è¡Œç»“æœå±•ç¤º)
  - [ä»»åŠ¡3ï¼šæ”¯æŒå‘é‡æœº](#ä»»åŠ¡3æ”¯æŒå‘é‡æœº)
    - [1.ä»»åŠ¡æè¿°](#1ä»»åŠ¡æè¿°-2)
    - [2.æ•°æ®é›†æè¿°](#2æ•°æ®é›†æè¿°-2)
    - [3.ä»»åŠ¡åˆ†æ](#3ä»»åŠ¡åˆ†æ-2)
      - [SVM](#svm)
    - [4.ä»£ç å®ç°ä¸åˆ†æ](#4ä»£ç å®ç°ä¸åˆ†æ-2)
      - [4.1 æ•°æ®è¯»å–ï¼Œé¢„å¤„ç†ä¸æ•°æ®é›†åˆ’åˆ†](#41-æ•°æ®è¯»å–é¢„å¤„ç†ä¸æ•°æ®é›†åˆ’åˆ†)
      - [4.2 SVMå®ç°](#42-svmå®ç°)
      - [4.3 è®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹](#43-è®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹)
      - [4.4 ä½œä¸šç»“æœå½•å…¥](#44-ä½œä¸šç»“æœå½•å…¥)
    - [5.è¿è¡Œç»“æœå±•ç¤º](#5è¿è¡Œç»“æœå±•ç¤º-1)



---

## ä»»åŠ¡1ï¼šå¤šé¡¹å¼å›å½’

### 1.ä»»åŠ¡æè¿°
å¤šé¡¹å¼å›å½’æ˜¯ä¸€ç§å›å½’åˆ†æå½¢å¼ï¼Œåœ¨è¿™ç§å½¢å¼ä¸­ï¼Œè‡ªå˜é‡ ( x ) å’Œå› å˜é‡ ( y ) ä¹‹é—´çš„å…³ç³»è¢«å»ºæ¨¡ä¸º ( n ) é˜¶å¤šé¡¹å¼ã€‚ä½¿ç”¨æœºå™¨å­¦ä¹ çš„æ–¹æ³•æ¥åˆ›å»ºä¸€ä¸ªå¤šé¡¹å¼å›å½’æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¯ä»¥æ ¹æ®ç»™å®šçš„æ•°æ®é›†é¢„æµ‹ç»“æœã€‚æ•°æ®é›†ç”±è‡ªå˜é‡ ( x ) å’Œå› å˜é‡ ( y ) ç»„æˆï¼Œä½ çš„ä»»åŠ¡æ˜¯æ‰¾åˆ°ä¸€ä¸ªå¤šé¡¹å¼ï¼Œèƒ½æœ€å¥½åœ°æè¿° ( x ) å’Œ ( y ) ä¹‹é—´çš„å…³ç³»ã€‚

### 2.æ•°æ®é›†æè¿°
æœ¬å®éªŒé€‰å–æ•°æ®é›†åŒ…å«125ä¸ªæ ·æœ¬ç‚¹ï¼Œæ¯ä¸ªæ ·æœ¬å…·æœ‰ä¸€ä¸ªè‡ªå˜é‡( x )å’Œä¸€ä¸ªå› å˜é‡( y )ã€‚æ•°æ®é›†æ ¹æ®4:1çš„æ¯”ä¾‹åˆ’åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ã€‚

### 3.ä»»åŠ¡åˆ†æ
å¤šé¡¹å¼å›å½’æ˜¯ä¸€ç§å›å½’åˆ†ææ–¹å¼ï¼Œé€šè¿‡å¤šé¡¹å¼å‡½æ•°æ¥æ‹Ÿåˆæ•°æ®é›†ï¼Œè¿™ç§æ–¹æ³•é€‚ç”¨äºæ•°æ®å’Œä¸€ä¸ªæˆ–å¤šä¸ªéçº¿æ€§å…³ç³»çš„æƒ…å†µã€‚å¤šé¡¹å¼å›å½’æ¨¡å‹çš„å½¢å¼å¦‚ä¸‹ï¼š
<div align=center>

$$
y = \beta_0+\beta_1x+\beta_2x^2+\beta_3x^3+...+\beta_nx^n+\epsilon 
$$

</div>

åœ¨å¤šé¡¹å¼å›å½’æ¨¡å‹ä¸­ï¼Œè‡ªå˜é‡çš„ä¸åŒæ¬¡æ–¹$x^2$,$x^3$,...$x^n$è¢«ä½œä¸ºæ–°ç‰¹å¾åŠ å…¥åˆ°æ¨¡å‹ä¸­ã€‚é€šè¿‡è®­ç»ƒæ•°æ®ï¼Œä½¿ç”¨æ™®é€šæœ€å°äºŒä¹˜æ³•ç­‰ç®—æ³•æ¥ä¼°è®¡å›å½’ç³»æ•°$\{ \beta_0,\beta_1,\beta_2...\beta_n\}$,ä»è€Œæ•æ‰æ•°æ®ä¸­çš„éçº¿æ€§å…³ç³»ã€‚

#### æ™®é€šæœ€å°äºŒä¹˜æ³•
æ™®é€šæœ€å°äºŒä¹˜æ³•æ˜¯ä¼°è®¡å¤šé¡¹å¼å›å½’æ¨¡å‹ç³»æ•°çš„å¸¸ç”¨æ–¹æ³•ï¼Œå…¶ç›®æ ‡æ˜¯æœ€å°åŒ–å®é™…å€¼ä¸é¢„æµ‹å€¼ä¹‹é—´çš„è¯¯å·®å¹³æ–¹å’Œï¼š
<div align=center>

$RSS = \sum_{i=1}^{m} (y_i - \hat{y}_i)^2$

mä¸ºæ ·æœ¬æ•°ï¼Œ$y_i$ ä¸ºå®é™…å€¼, $\hat{y}_i$ä¸ºé¢„æµ‹å€¼ã€‚é€šè¿‡æœ€å°åŒ–RSSï¼Œå°±å¯ä»¥æ‰¾åˆ°æœ€ä½³çš„å›å½’ç³»æ•°$\beta_0,\beta_1,\beta_2,...,\beta_n$ã€‚

</div>

### 4.ä»£ç å®ç°ä¸åˆ†æ
#### 4.1 æ•°æ®é›†è¯»å–ä¸æ•°æ®é¢„å¤„ç†

æ•°æ®é›†å­˜å‚¨åœ¨`homwework.csv`æ–‡ä»¶ä¸­ï¼Œä½¿ç”¨`numpy`åº“çš„`loadtxt`æ–¹æ³•æ¥è¯»å–æ•°æ®ã€‚ä¹‹ååˆ©ç”¨`reshape`å¢åŠ ä¸€ä¸ªç»´åº¦ä»¥æ–¹ä¾¿æ¨¡å‹è®­ç»ƒã€‚

```python
import numpy as np
from sklearn.preprocessing import PolynomialFeatures
import pandas as pd

train_data = np.loadtxt(
    open("ä½œä¸šä¸€/poly_reg/train_dataset.csv", "rb"), delimiter=",", skiprows=1
)
test_data = np.loadtxt(
    open("ä½œä¸šä¸€/poly_reg/test_dataset.csv", "rb"), delimiter=",", skiprows=1
)
new_test_data = pd.read_csv('ä½œä¸šä¸€/new_test.csv',header=0)


x_train = train_data[:, 0].reshape(-1, 1)
y_train = train_data[:, 1].reshape(-1, 1)
x_test = test_data[:, 0].reshape(-1, 1)
y_test = test_data[:, 1].reshape(-1, 1)
new_x_test=np.array(new_test_data['x'].values).reshape(-1,1)
```
æ¥ç€è®¾ç½®å¤šé¡¹å¼é˜¶æ•°ï¼Œå¹¶é‡‡ç”¨`sklearn.preprocessing`ä¸­çš„`PolynomialFeatures`æ–¹æ³•å¯¹è‡ªå˜é‡è¿›è¡Œç‰¹å¾æ‹“å±•ã€‚
```python
degree = 4

# å¤šé¡¹å¼ç‰¹å¾
poly_features = PolynomialFeatures(degree=degree)
x_train_poly = poly_features.fit_transform(x_train)
x_test_poly = poly_features.transform(x_test)
new_x_test_poly=poly_features.transform(new_x_test)
```

#### 4.2 æ¨¡å‹è®­ç»ƒ
é‡‡ç”¨`sklearn.linear_model`ä¸­çš„`LinearRegression`ä½œä¸ºæ¨¡å‹ï¼Œé€šè¿‡æ¨¡å‹çš„`fit()`æ–¹æ³•è¿›è¡Œè®­ç»ƒ

```python
# æ¨¡å‹è®­ç»ƒ
from sklearn.linear_model import LinearRegression

model = LinearRegression()
model.fit(x_train_poly, y_train)

```
#### 4.3 æ¨¡å‹è¯„ä»·ä¸é¢„æµ‹
ä½¿ç”¨æµ‹è¯•é›†å¯¹æ¨¡å‹è¿›è¡Œè¯„ä»·ï¼Œå¹¶å¯¹ä½œä¸šæ•°æ®é›†è¿›è¡Œé¢„æµ‹ï¼Œé€šè¿‡`sklearn.metrics`ä¸­çš„`mean_squared_error`è®¡ç®—æµ‹è¯•é›†çš„MSEï¼Œå°†ä½œä¸šæ•°æ®é›†çš„é¢„æµ‹ç»“æœç›´æ¥å­˜å‚¨å›åŸæ–‡ä»¶
```python
from sklearn.metrics import mean_squared_error

#æµ‹è¯•
train_predictions = model.predict(x_train_poly)
test_predictions = model.predict(x_test_poly)

#æ–¹å·®
train_mse= mean_squared_error(y_train,train_predictions)
test_mse = mean_squared_error(y_test,test_predictions)

print("è®­ç»ƒé›†æ–¹å·®:",train_mse)
print("æµ‹è¯•é›†æ–¹å·®",test_mse)

#ä½œä¸šæäº¤éƒ¨åˆ†
new_test_prediction=model.predict(new_x_test_poly)
#print(new_test_prediction)
new_test_data['y']=new_test_prediction
new_test_data.to_csv("ä½œä¸šä¸€/homework.csv",index=False)
```

#### 4.4 ç»“æœå¯è§†åŒ–
ä¸ºäº†èƒ½å¤Ÿæ›´åŠ ç›´è§‚çš„è§‚å¯Ÿæ¨¡å‹è®­ç»ƒæ•ˆæœï¼Œé‡‡ç”¨`matplotlib.pyplot`è¿›è¡Œå›¾åƒç»˜åˆ¶ï¼Œå¹¶ä¸”é€šè¿‡æ ¼å¼åŒ–è¾“å‡ºæ¥è¾“å‡ºæœ€ç»ˆçš„å›å½’æ›²çº¿è¡¨è¾¾å¼ã€‚
```python
import matplotlib.pyplot as plt

# ç»˜å›¾
plt.scatter(x_train, y_train, color='blue', label='Train data')
plt.scatter(x_test, y_test, color='red', label='Test data')

# ä¸ºäº†ç»˜åˆ¶å¹³æ»‘æ›²çº¿ï¼Œç”Ÿæˆè®¸å¤šç‚¹æ¥é¢„æµ‹
x_range = np.linspace(x_train.min(), x_train.max(), 500).reshape(-1, 1)
x_range_poly = poly_features.transform(x_range)
y_range_pred = model.predict(x_range_poly)

# è¾“å‡ºå¤šé¡¹å¼è¡¨è¾¾å¼ï¼ˆä»…åŒ…å«éé›¶ç³»æ•°çš„é¡¹ï¼‰
coefficients = model.coef_[0]
intercept = model.intercept_[0]

# æ„å»ºå¤šé¡¹å¼è¡¨è¾¾å¼å­—ç¬¦ä¸²
poly_expression = f"f(x) = {intercept:.2f}"
for i in range(1, degree + 1):
    if coefficients[i] != 0:
        poly_expression += f" + {coefficients[i]:.2f}x^{i}"

print("å¤šé¡¹å¼å›å½’æ¨¡å‹è¡¨è¾¾å¼ï¼š", poly_expression)

#ç»˜å›¾
plt.plot(x_range, y_range_pred, color='green', label=f'Polynomial Degree {degree}')
plt.title(poly_expression)
plt.xlabel('X')
plt.ylabel('Y')
plt.legend()
plt.savefig("ä½œä¸šä¸€/png_homework.png")
plt.show()
```


### 5.æµ‹è¯•ç»“æœå±•ç¤º
#### 5.1 ä¸åŒé˜¶æ•°ä¸‹çš„MSE
degree=2
![alt text](image.png)
degree=4
![alt text](image-1.png)
degree=6
![alt text](image-2.png)
degree=8
![alt text](image-3.png)

#### 5.2 ä¸åŒé˜¶æ•°ä¸‹çš„å›¾åƒæ›²çº¿ä»¥åŠè¡¨è¾¾å¼
degree=2
![alt text](img_1.png)
degree=4
![alt text](img_2.png)
degree=6
![alt text](img_3.png)
degree=8
![alt text](img_4.png)

---

## ä»»åŠ¡2ï¼šæ¦‚ç‡åˆ†ç±»æ³•
### 1.ä»»åŠ¡æè¿°
ä½¿ç”¨è´å¶æ–¯ä¼°è®¡æˆ–MLEï¼ˆæœ€å¤§ä¼¼ç„¶ä¼°è®¡ï¼‰ï¼Œæ¥é¢„æµ‹é¸¢å°¾èŠ±æ•°æ®é›†ä¸­èŠ±çš„ç§ç±»ã€‚
### 2.æ•°æ®é›†æè¿°
é¸¢å°¾èŠ±æ•°æ®é›†æ˜¯ç»Ÿè®¡å­¦å’Œæœºå™¨å­¦ä¹ ä¸­ç”¨äºåˆ†ç±»çš„ç»å…¸æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†åŒ…å«äº†ä¸‰ç§ä¸åŒçš„é¸¢å°¾èŠ±ï¼šSetosaã€Versicolorå’ŒVirginicaï¼Œæ¯ç§å„50ä¸ªæ ·æœ¬ã€‚æ¯ä¸ªæ ·æœ¬æœ‰å››ä¸ªå±æ€§ï¼šè¼ç‰‡é•¿åº¦ã€è¼ç‰‡å®½åº¦ã€èŠ±ç“£é•¿åº¦å’ŒèŠ±ç“£å®½åº¦ï¼Œæ‰€æœ‰çš„æµ‹é‡å•ä½éƒ½æ˜¯å˜ç±³ã€‚æ•°æ®é›†æ ¹æ®4:1çš„æ¯”ä¾‹åˆ’åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ã€‚æ¦‚ç‡åˆ†ç±»æ³•æ˜¯ä¸€ç§åŸºäºæ¦‚ç‡ç†è®ºçš„æ–¹æ³•ï¼Œé€‚åˆå¤„ç†æ­¤ç±»åˆ†ç±»é—®é¢˜ã€‚
### 3.ä»»åŠ¡åˆ†æ
è´å¶æ–¯ä¼°è®¡å’ŒMLEæ˜¯ç»Ÿè®¡å­¦ä¸­å¸¸ç”¨çš„ä¸¤ç§æ–¹æ³•ï¼Œç”¨äºå‚æ•°ä¼°è®¡ä¸åˆ†ç±»ä»»åŠ¡ã€‚å…¶ä¸­ï¼Œè´å¶æ–¯ä¼°è®¡åˆ©ç”¨å…ˆéªŒçŸ¥è¯†å’Œæ•°æ®æ¥è¿›è¡Œå‚æ•°ä¼°è®¡ï¼ŒMLEåˆ™æ˜¯é€šè¿‡æœ€å¤§åŒ–ä¼¼ç„¶å‚æ•°æ¥ä¼°è®¡å‚æ•°ã€‚è¿™ä¸¤ç§æ–¹æ³•éƒ½é€‚ç”¨äºåˆ†ç±»é—®é¢˜ï¼Œå¦‚æœ¬æ¬¡çš„é¸¢å°¾èŠ±æ•°æ®é›†åˆ†ç±»ã€‚

åœ¨æœ¬æ¬¡ä»»åŠ¡ä¸­ï¼Œæ•°æ®é›†å·²ç»æå‰å¤„ç†å¥½ï¼Œç•™ä¸‹äº†éœ€è¦çš„ç‰¹å¾å¹¶å­˜å‚¨äºcsvæ–‡ä»¶ä¸­ï¼Œåªéœ€è¦è¯»å–å¹¶åšç®€å•çš„é¢„å¤„ç†å³å¯ã€‚ä¹‹åè‡ªè¡Œå®ç°ä¸€ä¸ªé«˜æ–¯è´å¶æ–¯åˆ†ç±»å™¨ï¼Œè®­ç»ƒå¹¶è¯„ä¼°å³å¯
#### æ•°å­¦åŸºç¡€
##### è´å¶æ–¯ä¼°è®¡
è´å¶æ–¯ä¼°è®¡é€šè¿‡ç»“åˆå…ˆéªŒåˆ†å¸ƒå’Œä¼¼ç„¶å‡½æ•°æ¥ä¼°è®¡å‚æ•°ã€‚è´å¶æ–¯å…¬å¼å¦‚ä¸‹ï¼š
<div align = center>

$P(\theta|X)=\frac{P(X|\theta)P(\theta)}{P(X)}$

</div>  

å…¶ä¸­ï¼Œ$P(\theta|X)$ æ˜¯ç»™å®šæ•°æ® $X$ åå‚æ•° $\theta$ çš„åéªŒæ¦‚ç‡ï¼›$P(X|\theta)$ æ˜¯ç»™å®šå‚æ•° $\theta$ åæ•°æ® $X$ çš„ä¼¼ç„¶å‡½æ•°ï¼›$P(\theta)$ æ˜¯å‚æ•° $\theta$ çš„æ¦‚ç‡ï¼›$P(X)$ æ˜¯æ•°æ®Xçš„æ¦‚ç‡
##### æœ€å¤§ä¼¼ç„¶ä¼°è®¡
æœ€å¤§ä¼¼ç„¶ä¼°è®¡é€šè¿‡æœ€å¤§åŒ–ä¼¼ç„¶å‡½æ•°æ¥ä¼°è®¡å‚æ•°ã€‚ä¼¼ç„¶å‡½æ•°è¡¨ç¤ºç»™å®šå‚æ•° $\theta$ æ—¶æ•°æ®å‡ºç°çš„æ¦‚ç‡ã€‚æœ€å¤§ä¼¼ç„¶ä¼°è®¡å°±æ˜¯è¦æ‰¾åˆ°ä½¿å¾—ä¼¼ç„¶å‡½æ•°æœ€å¤§çš„å‚æ•° $\theta$. ä¼¼ç„¶å‡½æ•°å¦‚ä¸‹
<div align=center>

$ ğ¿(ğœƒ|ğ‘‹)=ğ‘ƒ(ğ‘‹|ğœƒ)$

</div>

### 4.ä»£ç å®ç°ä¸åˆ†æ
#### 4.1 æ•°æ®é›†è¯»å–ä¸åˆ’åˆ†
ä½¿ç”¨`sklearn.datasets`ä¸­çš„`load_iris()`åŠ è½½é¸¢å°¾èŠ±æ•°æ®é›†ï¼Œå°†ç‰¹å¾æ•°æ®`X`å’Œç›®æ ‡æ ‡ç­¾`Y`åˆ†ç¦»ï¼Œå¹¶åˆ©ç”¨`sklearn.model_selection`ä¸­çš„`train_test_split`è¿›è¡Œè®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„åˆ’åˆ†ã€‚

```python
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split

iris = load_iris()
X = iris.data
Y = iris.target

# åˆ’åˆ†æ•°æ®é›†
x_train, x_val, y_train, y_val = train_test_split(
    X, Y, test_size=0.2, random_state=1919
)

```

#### 4.2 å®šä¹‰é«˜æ–¯æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨ç±»
å®šä¹‰ä¸€ä¸ªé«˜æ–¯æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨ä½œä¸ºæ¨¡å‹ï¼Œå…¶ä¸­`fit()`æ–¹æ³•è¿›è¡Œäº†æ¯ä¸ªç±»å‡å€¼ï¼Œæ–¹å·®ï¼Œå…ˆéªŒæ¦‚ç‡çš„è®¡ç®—ä»¥ä½œä¸ºé¢„æµ‹ä¾æ®ã€‚`_calculate_likehlihood()`æ–¹æ³•ä¸­è®¡ç®—äº†ç±»çš„ç‰¹å¾çš„ä¼¼ç„¶ï¼Œå‡è®¾å®ƒç¬¦åˆé«˜æ–¯åˆ†å¸ƒï¼Œ`_calculate_posterior()`æ–¹æ³•å¯¹è¾“å…¥æ•°æ®è¿›è¡ŒåéªŒæ¦‚ç‡çš„è®¡ç®—ï¼Œå¹¶ä¸”è¿”å›æ¦‚ç‡æœ€é«˜çš„é¢„æµ‹ç±»ã€‚æœ€åå®šä¹‰äº†`predict()`æ–¹æ³•æ¥æ”¶æ•°æ®å¹¶è¿”å›é¢„æµ‹ç»“æœã€‚

```python
import numpy as np

class GaussianNaiveBayes:
    def fit(self, x, y):
        # æ‰€æœ‰target
        self.classes = np.unique(y)

        # åˆå§‹åŒ–å‡å€¼ï¼Œæ–¹å·®ï¼Œå…ˆéªŒæ¦‚ç‡
        self.mean = np.zeros((len(self.classes), x.shape[1]), dtype=np.float64)
        self.var = np.zeros((len(self.classes), x.shape[1]), dtype=np.float64)
        self.priors = np.zeros(len(self.classes), dtype=np.float64)

        # è®¡ç®—æ¯ä¸ªç±»çš„å‡å€¼ï¼Œæ–¹å·®å’Œå…ˆéªŒæ¦‚ç‡
        for idx, c in enumerate(self.classes):
            x_c = x[y == c]
            self.mean[idx, :] = x_c.mean(axis=0)
            self.var[idx, :] = x_c.var(axis=0)
            self.priors[idx] = x_c.shape[0] / float(x.shape[0])

    def _calculate_likehlihood(self, class_idx, x):
        # è®¡ç®—ç»™å®šç±»ä¸‹ç‰¹å¾çš„ä¼¼ç„¶ï¼ˆå‡è®¾ä¼¼ç„¶æœä»é«˜æ–¯åˆ†å¸ƒ)
        mean = self.mean[class_idx]
        var = self.mean[class_idx]
        numerator = np.exp(-((x - mean) ** 2) / (2 * var))  # é«˜æ–¯åˆ†å¸ƒçš„åˆ†å­éƒ¨åˆ†
        denominator = np.sqrt(2 * np.pi * var)  # åˆ†æ¯éƒ¨åˆ†
        return numerator / denominator

    def _calculate_posterior(self, x):
        # è®¡ç®—åéªŒæ¦‚ç‡ï¼Œå¹¶è¿”å›å…·æœ‰æœ€é«˜åéªŒæ¦‚ç‡çš„ç±»
        posteriors = []
        for idx, c in enumerate(self.classes):
            piror = np.log(self.priors[idx])  # å…ˆéªŒæ¦‚ç‡å–å¯¹æ•°
            conditional = np.sum(
                np.log(self._calculate_likehlihood(idx, x))
            )  # æ¡ä»¶æ¦‚ç‡å–å¯¹æ•°å¹¶æ±‚å’Œ
            posterior = piror + conditional  # è®¡ç®—åéªŒæ¦‚ç‡
            posteriors.append(posterior)
        return self.classes[np.argmax(posteriors)]  # è¿”å›åéªŒæ¦‚ç‡æœ€é«˜çš„ç±»

    def predict(self, X):
        y_pred = [self._calculate_posterior(x) for x in X]
        return np.array(y_pred)
```

#### 4.3 è®­ç»ƒæ¨¡å‹å¹¶è¯„ä¼°
å¯¹è‡ªå®šä¹‰çš„é«˜æ–¯æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨è¿›è¡Œè®­ç»ƒï¼Œå¹¶ç”¨æµ‹è¯•é›†æ•°æ®å¯¹å…¶è¿›è¡Œè¯„ä¼°ï¼Œé‡‡ç”¨`sklearn.metrics`ä¸­çš„`accuracy_score`æ–¹æ³•è®¡ç®—è¯„ä¼°çš„accuracyã€‚

```python
from sklearn.metrics import accuracy_score

model = GaussianNaiveBayes()
model.fit(x_train,y_train)
y_pred = model.predict(x_val)
accuracy=accuracy_score(y_val,y_pred)
print(f"æ¨¡å‹å‡†ç¡®ç‡:{accuracy}")

```

#### 4.4 æµ‹è¯•æ¨¡å‹å¹¶ä¿å­˜ç»“æœ
ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹å¯¹ä½œä¸šæ•°æ®è¿›è¡Œé¢„æµ‹ï¼Œå¹¶ä¿å­˜é¢„æµ‹ç»“æœåˆ°åŸæ–‡ä»¶ä¸­
```python
import pandas as pd

# æµ‹è¯•ç¯èŠ‚
test_iris = pd.read_csv("hmwk2/iris_test.csv", header=0)

test_data = np.array(test_iris[test_iris.columns[:4]].values).reshape(-1, 4)
# print(test_data)
pred_test_target = model.predict(test_data)
#print(pred_test_target)
test_iris["species"]=pred_test_target

test_iris.to_csv("hmwk2/homework.csv",index=False)
```

### 5.è¿è¡Œç»“æœå±•ç¤º
æœ€ç»ˆè¯„ä¼°çš„æ­£ç¡®ç‡ä¸º`0.96`ï¼Œæ ¹æ®åŠ©æ•™åé¦ˆä¿¡æ¯ï¼Œå¯¹ä½œä¸šé›†çš„æ­£ç¡®ç‡ä¸º`0.867`
![alt text](image-4.png)

---

## ä»»åŠ¡3ï¼šæ”¯æŒå‘é‡æœº
### 1.ä»»åŠ¡æè¿°
åŸºäºSVMæ„å»ºæœºå™¨å­¦ä¹ æ¨¡å‹, å¯¹Omniglotä¸­çš„å­—ç¬¦åˆ†ç±»
### 2.æ•°æ®é›†æè¿°
Omniglot Datasetæ˜¯å…¨è¯­è¨€æ–‡å­—æ•°æ®é›†ï¼ŒåŒ…å«å„ç§è¯­è¨€çš„ä¸åŒå­—æ¯è¡¨ï¼Œå¦‚æ—¥è¯­çš„å¹³å‡åå’Œç‰‡å‡åï¼ŒéŸ©è¯­çš„å…ƒéŸ³å’Œè¾…éŸ³ï¼Œæœ€å¸¸è§çš„æ‹‰ä¸å­—æ¯abcdç­‰ã€‚Omniglot Datasetå…±åŒ…å«50ä¸ªä¸åŒè¯­è¨€çš„å­—æ¯è¡¨ï¼Œæ¯ä¸ªå­—æ¯è¡¨ä¸­åŒ…å«ä¸åŒçš„å­—ç¬¦ï¼Œå…±1623ç§å­—ç¬¦ï¼Œæ¯ç§å­—ç¬¦ç”±20ä¸ªä¸åŒçš„äººä¹¦å†™ã€‚æœ¬æ¬¡å®éªŒé€‰å–å…¶ä¸­200ç§å­—ç¬¦ï¼Œæ¯ç§å­—ç¬¦çš„15å¼ å›¾ç‰‡ä½œä¸ºè®­ç»ƒé›†ï¼Œ5å¼ å›¾ç‰‡ä½œä¸ºæµ‹è¯•é›†ã€‚
### 3.ä»»åŠ¡åˆ†æ
æœ¬æ¬¡ä»»åŠ¡è¦æ±‚ä½¿ç”¨SVMå¯¹Omniglotæ•°æ®é›†ä¸­çš„å­—ç¬¦è¿›è¡Œåˆ†ç±»ã€‚
#### SVM
SVMï¼Œä¹Ÿå°±æ˜¯æ”¯æŒå‘é‡æœºï¼Œæ˜¯ä¸€ç§ç›‘ç£å­¦ä¹ æ¨¡å‹ï¼Œç”¨äºåˆ†ç±»å’Œå›å½’åˆ†æã€‚SVMçš„ç›®æ ‡æ˜¯æ‰¾åˆ°ä¸€ä¸ªèƒ½å¤Ÿæœ€å¥½åˆ†ç¦»ä¸åŒç±»åˆ«çš„è¶…å¹³é¢ã€‚
æ•°æ®ä¸­æœ€é‡è¦çš„éƒ¨åˆ†æ˜¯æ”¯æŒå‘é‡ï¼Œä¹Ÿå°±æ˜¯è·ç¦»ç›®æ ‡è¶…å¹³é¢æœ€è¿‘çš„é‚£äº›æ•°æ®ç‚¹ã€‚è¿™äº›æ•°æ®å¯¹ç¡®å®šè¶…å¹³é¢æœ‰ç€å†³å®šæ€§çš„ä½œç”¨ã€‚
SVMä¸ä»…è¦æ‰¾åˆ°ä¸€ä¸ªè¶…å¹³é¢ï¼Œè¿˜è¦ä½¿è¿™ä¸ªè¶…å¹³é¢ä¸æ”¯æŒå‘é‡çš„é—´éš”æœ€å¤§åŒ–ä»¥æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚
å½“æ•°æ®æ˜¯çº¿æ€§å¯åˆ†çš„ï¼Œä¹Ÿå°±æ˜¯å¯ä»¥é€šè¿‡ä¸€ä¸ªçº¿æ€§è¶…å¹³é¢å®Œå…¨åˆ†å¼€ï¼Œé‚£ä¹ˆSVMå¯ä»¥ç›´æ¥æ‰¾åˆ°ä¸€ä¸ªåˆé€‚çš„è¶…å¹³é¢ã€‚ä½†æ˜¯å¦‚æœæ•°æ®çº¿æ€§ä¸å¯åˆ†ï¼Œå°±è¦é€šè¿‡æ ¸`kernel`æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚åœ¨æœ¬æ¬¡ä»»åŠ¡ä¸­è®¡åˆ’ä½¿ç”¨RBFæ ¸çŸ©é˜µï¼Œä¹Ÿå°±æ˜¯é«˜æ–¯æ ¸ï¼Œå®ƒå…·æœ‰è¾ƒå¼ºçš„çµæ´»æ€§å’Œé€‚åº”æ€§ï¼Œå¸¸ç”¨äºéçº¿æ€§æ•°æ®ã€‚å…¶å…¬å¼å¦‚ä¸‹
<div align=center>

$K(x,x')=\exp(-\gamma\|x-x'\|^2)$

</div>

$x$å’Œ$x'$æ˜¯ä¸¤ä¸ªæ ·æœ¬ç‚¹ï¼Œ$\|x-x'\|$æ˜¯ä¸¤ä¸ªæ ·æœ¬é—´çš„æ¬§å‡ é‡Œå¾—è·ç¦»ï¼Œ$\gamma$æ˜¯ç”¨äºæ§åˆ¶æ ¸å‡½æ•°å®½åº¦çš„è¶…å‚æ•°
åœ¨æœ¬æ¬¡ä»»åŠ¡ä¸­ï¼Œå…ˆè¿›è¡Œæ•°æ®è¯»å–ä¸é¢„å¤„ç†ï¼Œä¹‹åè‡ªå®šä¹‰ä¸€ä¸ªSVMï¼Œç”±äºæœ¬æ¬¡æ˜¯å¤šç±»åˆ†ç±»ä»»åŠ¡ï¼Œè€ŒSVMæœ¬è´¨æ˜¯ä¸€ä¸ªäºŒåˆ†ç±»æ¨¡å‹ï¼Œå› æ­¤æœ¬æ¬¡å‡†å¤‡é‡‡ç”¨â€œä¸€å¯¹ä¸€â€ç­–ç•¥ä»¥ä½¿SVMå¯ä»¥è§£å†³å¤šåˆ†ç±»é—®é¢˜ã€‚
æ‰€è°“â€œä¸€å¯¹ä¸€â€æ˜¯æŒ‡å¯¹äºæ¯ä¸¤ä¸ªä¸åŒçš„ç±»åˆ«éƒ½è®­ç»ƒä¸€ä¸ªSVMåˆ†ç±»å™¨ï¼Œæœ€ç»ˆé¢„æµ‹ç±»åˆ«ç”±æ‰€æœ‰çš„åˆ†ç±»å™¨æŠ•ç¥¨å†³å®šã€‚è¿™ç§æ–¹å¼æ›´åŠ å‡†ç¡®ï¼Œä½†æ˜¯è®¡ç®—å¼€é”€æ¯”è¾ƒå¤§
### 4.ä»£ç å®ç°ä¸åˆ†æ
#### 4.1 æ•°æ®è¯»å–ï¼Œé¢„å¤„ç†ä¸æ•°æ®é›†åˆ’åˆ†
é€šè¿‡`os.path`ä¸­çš„å¤šä¸ªæ–¹æ³•ç¡®å®šæ•°æ®æ‰€åœ¨çš„è·¯å¾„ä»¥ä¿è¯åœ¨ä¸åŒç¯å¢ƒä¸‹ç¨‹åºéƒ½å¯ä»¥æ­£å¸¸è¯»å–æ•°æ®ã€‚ç”±äºæ•°æ®å­˜å‚¨åœ¨matæ–‡ä»¶ä¸­ï¼Œæ•…ä½¿ç”¨`scipy.io`ä¸­çš„`loadmat`æ–¹æ³•è¿›è¡Œæ•°æ®åŠ è½½ã€‚
é¢„å¤„ç†ç¯èŠ‚ä¸­ï¼Œå…ˆå°†æ•°æ®ä»28*28å±•å¹³ä¸ºé•¿784çš„ä¸€ç»´å‘é‡ï¼Œä¹‹åæ ¹æ®æ•°æ®é›†ä»‹ç»ï¼Œæ‰‹åŠ¨åˆ›å»ºæ ‡ç­¾1~200ï¼Œæ¯ä¸ªæ ‡ç­¾åŒ…å«15ä¸ªæ ·æœ¬ã€‚æœ€åå€ŸåŠ©`sklearn.preprocessing`ä¸­çš„`MinMaxScaler`æ–¹æ³•è¿›è¡Œæ•°æ®æ ‡å‡†åŒ–ä»¥å¢å¼ºæ¨¡å‹è®­ç»ƒæ•ˆæœã€‚æœ€åé‡‡ç”¨`sklearn.model_selection`ä¸­çš„`train_test_split`æ–¹æ³•è¿›è¡Œæ•°æ®é›†åˆ’åˆ†ã€‚

```python
import scipy.io as sio
import numpy as np
import os
import pandas as pd
from  import train_test_split
from sklearn.preprocessing import MinMaxScaler

# åŠ è½½æ•°æ®
current_dir = os.path.dirname(os.path.abspath(__file__))
train_data_path = os.path.join(current_dir, "train_data.mat")
test_data_path = os.path.join(current_dir, "test_data.mat")

train_data = sio.loadmat(train_data_path)
test_data = sio.loadmat(test_data_path)

# å°†è®­ç»ƒæ•°æ®å±•å¼€ä¸º 28*28 = 784 çš„ä¸€ç»´å‘é‡
x_train = train_data["train"].reshape(-1, 28 * 28)
# åˆ›å»ºæ ‡ç­¾ 1-200, æ¯ä¸ªæ ‡ç­¾15ä¸ªæ ·æœ¬
y_train = np.repeat(np.arange(1, 201), 15)
# å°†æµ‹è¯•æ•°æ®å±•å¼€ä¸º 28*28 = 784 çš„ä¸€ç»´å‘é‡
x_test = test_data["test"].reshape(-1, 28 * 28)

# æ•°æ®æ ‡å‡†åŒ–
scaler = MinMaxScaler()
x_train = scaler.fit_transform(x_train)
x_test = scaler.transform(x_test)

# åˆ’åˆ†æ•°æ®é›†
x_train, x_val, y_train, y_val = train_test_split(
    x_train, y_train, test_size=0.25, stratify=y_train, random_state=1919
)
```
#### 4.2 SVMå®ç°
å®ç°ä¸€ä¸ªä¸€å¯¹ä¸€çš„SVMæ¨¡å‹ï¼Œæ ¸å‡½æ•°ä½¿ç”¨RBFã€‚
åœ¨`rbf_kernel`æ–¹æ³•ä¸­è¿›è¡ŒRBFæ ¸çŸ©é˜µçš„è®¡ç®—ã€‚
`objective`æ–¹æ³•è®¡ç®—äº†SVMçš„å¯¹å¶å½¢å¼ï¼Œç”¨äºåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é€šè¿‡ä¼˜åŒ–æ‹‰æ ¼æœ—æ—¥ä¹˜å­$\alpha$ä»¥æ‰¾åˆ°æœ€ä¼˜çš„åˆ†ç±»å†³ç­–è¾¹ç•Œï¼Œå…¶å…¬å¼ä¸º

<div align=center>

$ L(\alpha)=\sum_{i=1}^N\alpha_i-\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_jK(x_i,x_j)$

</div>

`zerofun`æ–¹æ³•å®šä¹‰äº†çº¦æŸæ¡ä»¶ï¼Œä½¿å¾—æ‹‰æ ¼æœ—æ—¥ä¹˜å­çš„åŠ æƒå’Œä¸º0ã€‚`fit`æ–¹æ³•ä½¿ç”¨äºŒåˆ†ç±»SVMé‡‡ç”¨ä¸€å¯¹ä¸€ç­–ç•¥è®­ç»ƒæ¯å¯¹ç±»çš„åˆ†ç±»å™¨ã€‚`predict`æ–¹æ³•ç”¨äºé¢„æµ‹æ–°çš„æ•°æ®ç‚¹çš„ç±»åˆ«ï¼Œé€šè¿‡æŠ•ç¥¨æœºåˆ¶ç¡®å®šæœ€ç»ˆåˆ†ç±»ç»“æœ
```python
from scipy.optimize import minimize
import itertools
# å®ç°SVM
class NonlinearSVM:
    def __init__(self, C=1.0, gamma=0.05):
        self.C = C# æ­£åˆ™åŒ–å‚æ•°
        self.gamma = gamma# RBFæ ¸å‚æ•°
        self.classifiers = {}# å­˜å‚¨æ¯å¯¹ç±»çš„åˆ†ç±»å™¨

    def rbf_kernel(self, X1, X2):#è®¡ç®—RBFæ ¸çŸ©é˜µ
        K = np.exp(-self.gamma * np.linalg.norm(X1[:, np.newaxis] - X2, axis=2) ** 2)
        return K

    def objective(self, alpha, y, K):  # SVMå¯¹å¶çš„ç›®æ ‡å‡½æ•°ã€‚
        return 0.5 * np.sum(
            alpha * alpha[:, np.newaxis] * y * y[:, np.newaxis] * K
        ) - np.sum(alpha)

    def zerofun(self, alpha, y):  # çº¦æŸæ¡ä»¶
        return np.dot(alpha, y)

    def fit(self, X, y):
        classes = np.unique(y)
        for i, j in itertools.combinations(classes, 2):# å¯¹æ¯å¯¹ç±»åˆ«è®­ç»ƒä¸€ä¸ªåˆ†ç±»å™¨
            print(f"Training for classes {i} vs {j}")
            idx = np.where((y == i) | (y == j))[0]# è·å–å±äºè¿™ä¸¤ä¸ªç±»åˆ«çš„æ ·æœ¬
            X_ij, y_ij = X[idx], y[idx]
            y_ij = np.where(y_ij == i, 1, -1)# å°†æ ‡ç­¾è½¬æ¢ä¸º+1å’Œ-1

            K = self.rbf_kernel(X_ij, X_ij)# è®¡ç®—RBFæ ¸çŸ©é˜µ

            N = len(y_ij)
            alpha0 = np.zeros(N)# åˆå§‹åŒ–æ‹‰æ ¼æœ—æ—¥ä¹˜å­
            B = [(0, self.C) for _ in range(N)]# å®šä¹‰alphaçš„è¾¹ç•Œ
            constraints = {"type": "eq", "fun": lambda alpha: self.zerofun(alpha, y_ij)}# å®šä¹‰çº¦æŸæ¡ä»¶
            
            # æœ€å°åŒ–ç›®æ ‡å‡½æ•°
            res = minimize(
                self.objective,
                alpha0,
                args=(y_ij, K),
                bounds=B,
                constraints=constraints,
            )
            alpha = res.x

            sv_idx = np.where(alpha > 1e-5)[0]
            b = np.mean([y_ij[k] - np.sum(alpha * y_ij * K[k]) for k in sv_idx])

            self.classifiers[(i, j)] = (alpha, b, X_ij, y_ij)

    def predict(self, X):
        votes = np.zeros((len(X), len(self.classifiers)))
        for k, ((i, j), (alpha, b, X_train, y_train)) in enumerate(
            self.classifiers.items()
        ):
            print(f"Test for classes {i} vs {j}")
            K = self.rbf_kernel(X_train, X)
            predictions = K.T @ (alpha * y_train) + b
            votes[:, k] = np.where(predictions > 0, i, j)
        y_pred = np.apply_along_axis(
            lambda x: np.bincount(x.astype(int)).argmax(), axis=1, arr=votes
        )
        return y_pred

```

#### 4.3 è®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹
ä½¿ç”¨è‡ªå®šä¹‰çš„SVMæ¨¡å‹çš„`fit`æ–¹æ³•è¿›è¡Œæ¨¡å‹è®­ç»ƒï¼Œå¹¶è®¡ç®—åœ¨æµ‹è¯•é›†ä¸Šçš„æ­£ç¡®ç‡
```python
from sklearn.metrics import accuracy_score

# è®­ç»ƒSVMæ¨¡å‹
svm = NonlinearSVM()
print("å¼€å§‹è®­ç»ƒ")
svm.fit(x_train, y_train)

# è®¡ç®—å‡†ç¡®ç‡
print("è¿›è¡Œå‡†ç¡®ç‡è®¡ç®—")
y_val_pred = svm.predict(x_val)
accuracy = accuracy_score(y_val, y_val_pred)
print("å‡†ç¡®ç‡:", accuracy)

```

#### 4.4 ä½œä¸šç»“æœå½•å…¥
ä½¿ç”¨æ¨¡å‹çš„`predict`æ–¹æ³•å¯¹ä½œä¸šæ•°æ®é›†è¿›è¡Œé¢„æµ‹ï¼Œå¹¶å°†é¢„æµ‹ç»“æœå­˜å‚¨å›åŸæ–‡ä»¶ã€‚
```python
# ä½œä¸šéƒ¨åˆ†
print("è¿›è¡Œä½œä¸šç»“æœè®¡ç®—ä¸å½•å…¥")
y_test_pred = svm.predict(x_test)
hmwk_csv = pd.read_csv(os.path.join(current_dir, "submission.csv"), header=0)
hmwk_csv["é¢„æµ‹ç»“æœ"] = y_test_pred

hmwk_csv.to_csv(os.path.join(current_dir, "submission.csv"), index=False)
input()

```
### 5.è¿è¡Œç»“æœå±•ç¤º