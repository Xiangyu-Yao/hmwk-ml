# æ¨¡å¼è¯†åˆ«ä¸æœºå™¨å­¦ä¹ æŠ¥å‘Š
| å§“å   | å­¦å·       | ç­çº§     |
| ------ | ---------- | -------- |
| å§šç¿”å®‡ | 2021302474 | 10042101 |

</div>  

- [æ¨¡å¼è¯†åˆ«ä¸æœºå™¨å­¦ä¹ æŠ¥å‘Š](#æ¨¡å¼è¯†åˆ«ä¸æœºå™¨å­¦ä¹ æŠ¥å‘Š)
  - [ä»»åŠ¡1ï¼šå¤šé¡¹å¼å›å½’](#ä»»åŠ¡1å¤šé¡¹å¼å›å½’)
    - [1.ä»»åŠ¡æè¿°](#1ä»»åŠ¡æè¿°)
    - [2.æ•°æ®é›†æè¿°](#2æ•°æ®é›†æè¿°)
    - [3.ä»»åŠ¡åˆ†æ](#3ä»»åŠ¡åˆ†æ)
      - [æ™®é€šæœ€å°äºŒä¹˜æ³•](#æ™®é€šæœ€å°äºŒä¹˜æ³•)
    - [4.ä»£ç å®ç°ä¸åˆ†æ](#4ä»£ç å®ç°ä¸åˆ†æ)
      - [4.1 æ•°æ®é›†è¯»å–ä¸æ•°æ®é¢„å¤„ç†](#41-æ•°æ®é›†è¯»å–ä¸æ•°æ®é¢„å¤„ç†)
      - [4.2 æ¨¡å‹è®­ç»ƒ](#42-æ¨¡å‹è®­ç»ƒ)
      - [4.3 æ¨¡å‹è¯„ä»·ä¸é¢„æµ‹](#43-æ¨¡å‹è¯„ä»·ä¸é¢„æµ‹)
      - [4.4 ç»“æœå¯è§†åŒ–](#44-ç»“æœå¯è§†åŒ–)
    - [5.æµ‹è¯•ç»“æœå±•ç¤º](#5æµ‹è¯•ç»“æœå±•ç¤º)
      - [5.1 ä¸åŒé˜¶æ•°ä¸‹çš„MSE](#51-ä¸åŒé˜¶æ•°ä¸‹çš„mse)
      - [5.2 ä¸åŒé˜¶æ•°ä¸‹çš„å›¾åƒæ›²çº¿ä»¥åŠè¡¨è¾¾å¼](#52-ä¸åŒé˜¶æ•°ä¸‹çš„å›¾åƒæ›²çº¿ä»¥åŠè¡¨è¾¾å¼)
  - [ä»»åŠ¡2ï¼šæ¦‚ç‡åˆ†ç±»æ³•](#ä»»åŠ¡2æ¦‚ç‡åˆ†ç±»æ³•)
    - [1.ä»»åŠ¡æè¿°](#1ä»»åŠ¡æè¿°-1)
    - [2.æ•°æ®é›†æè¿°](#2æ•°æ®é›†æè¿°-1)
    - [3.ä»»åŠ¡åˆ†æ](#3ä»»åŠ¡åˆ†æ-1)
      - [æ•°å­¦åŸºç¡€](#æ•°å­¦åŸºç¡€)
        - [è´å¶æ–¯ä¼°è®¡](#è´å¶æ–¯ä¼°è®¡)
        - [æœ€å¤§ä¼¼ç„¶ä¼°è®¡](#æœ€å¤§ä¼¼ç„¶ä¼°è®¡)
    - [4.ä»£ç å®ç°ä¸åˆ†æ](#4ä»£ç å®ç°ä¸åˆ†æ-1)
      - [4.1 æ•°æ®é›†è¯»å–ä¸åˆ’åˆ†](#41-æ•°æ®é›†è¯»å–ä¸åˆ’åˆ†)
      - [4.2 å®šä¹‰é«˜æ–¯æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨ç±»](#42-å®šä¹‰é«˜æ–¯æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨ç±»)
      - [4.3 è®­ç»ƒæ¨¡å‹å¹¶è¯„ä¼°](#43-è®­ç»ƒæ¨¡å‹å¹¶è¯„ä¼°)
      - [4.4 æµ‹è¯•æ¨¡å‹å¹¶ä¿å­˜ç»“æœ](#44-æµ‹è¯•æ¨¡å‹å¹¶ä¿å­˜ç»“æœ)
    - [5.è¿è¡Œç»“æœå±•ç¤º](#5è¿è¡Œç»“æœå±•ç¤º)
  - [ä»»åŠ¡3ï¼šæ”¯æŒå‘é‡æœº](#ä»»åŠ¡3æ”¯æŒå‘é‡æœº)
    - [1.ä»»åŠ¡æè¿°](#1ä»»åŠ¡æè¿°-2)
    - [2.æ•°æ®é›†æè¿°](#2æ•°æ®é›†æè¿°-2)
    - [3.ä»»åŠ¡åˆ†æ](#3ä»»åŠ¡åˆ†æ-2)
      - [SVM](#svm)
    - [4.ä»£ç å®ç°ä¸åˆ†æ](#4ä»£ç å®ç°ä¸åˆ†æ-2)
      - [4.1 æ•°æ®è¯»å–ï¼Œé¢„å¤„ç†ä¸æ•°æ®é›†åˆ’åˆ†](#41-æ•°æ®è¯»å–é¢„å¤„ç†ä¸æ•°æ®é›†åˆ’åˆ†)
      - [4.2 SVMå®ç°](#42-svmå®ç°)
      - [4.3 è®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹](#43-è®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹)
      - [4.4 ä½œä¸šç»“æœå½•å…¥](#44-ä½œä¸šç»“æœå½•å…¥)
    - [5.è¿è¡Œç»“æœå±•ç¤º](#5è¿è¡Œç»“æœå±•ç¤º-1)
  - [ä»»åŠ¡4ï¼šå†³ç­–æ ‘](#ä»»åŠ¡4å†³ç­–æ ‘)
    - [1.ä»»åŠ¡æè¿°](#1ä»»åŠ¡æè¿°-3)
    - [2.æ•°æ®é›†æè¿°](#2æ•°æ®é›†æè¿°-3)
    - [3.ä»»åŠ¡åˆ†æ](#3ä»»åŠ¡åˆ†æ-3)
      - [å†³ç­–æ ‘](#å†³ç­–æ ‘)
      - [ä»£ç æ€è·¯åˆ†æ](#ä»£ç æ€è·¯åˆ†æ)
    - [4.ä»£ç å®ç°å’Œåˆ†æ](#4ä»£ç å®ç°å’Œåˆ†æ)
      - [4.1 æ•°æ®åŠ è½½å’Œé¢„å¤„ç†](#41-æ•°æ®åŠ è½½å’Œé¢„å¤„ç†)
      - [4.2 å†³ç­–æ ‘å®ç°](#42-å†³ç­–æ ‘å®ç°)
      - [4.3 æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°](#43-æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°)
      - [4.4 ä½œä¸šç»“æœå½•å…¥](#44-ä½œä¸šç»“æœå½•å…¥-1)
    - [5.ç»“æœå±•ç¤º](#5ç»“æœå±•ç¤º)
  - [ä»»åŠ¡5ï¼šå²¸è¾¹åƒåœ¾è¯†åˆ«](#ä»»åŠ¡5å²¸è¾¹åƒåœ¾è¯†åˆ«)
    - [1.ä»»åŠ¡æè¿°](#1ä»»åŠ¡æè¿°-4)
    - [2.æ•°æ®é›†åˆ†æ](#2æ•°æ®é›†åˆ†æ)
    - [3.ä»»åŠ¡åˆ†æ](#3ä»»åŠ¡åˆ†æ-4)
      - [deeplabv3](#deeplabv3)
    - [4.ä»£ç å®ç°å’Œåˆ†æ](#4ä»£ç å®ç°å’Œåˆ†æ-1)
      - [4.1å…¨å±€è®¾ç½®](#41å…¨å±€è®¾ç½®)
      - [4.2 æ•°æ®è¯»å–å’Œé¢„å¤„ç†](#42-æ•°æ®è¯»å–å’Œé¢„å¤„ç†)
      - [4.3æ¨¡å‹è®­ç»ƒ](#43æ¨¡å‹è®­ç»ƒ)
      - [4.4 æ¨¡å‹æµ‹è¯•å’Œç»“æœå¯è§†åŒ–](#44-æ¨¡å‹æµ‹è¯•å’Œç»“æœå¯è§†åŒ–)
      - [4.5 ç¨‹åºæ€»é€»è¾‘](#45-ç¨‹åºæ€»é€»è¾‘)
      - [4.6 æ¥å£å®šä¹‰](#46-æ¥å£å®šä¹‰)
    - [5.ç»“æœå±•ç¤º](#5ç»“æœå±•ç¤º-1)
      - [æœ€ç»ˆæ¨¡å‹çš„ç»“æœå¯è§†åŒ–](#æœ€ç»ˆæ¨¡å‹çš„ç»“æœå¯è§†åŒ–)
      - [è®­ç»ƒè®°å½•ï¼š](#è®­ç»ƒè®°å½•)
      - [æ‰“æ¦œè®°å½•ï¼š](#æ‰“æ¦œè®°å½•)
      - [æ‰“æ¦œæ’å](#æ‰“æ¦œæ’å)
      - [æ¨¡å‹æµ‹è¯•è®°å½•ï¼š](#æ¨¡å‹æµ‹è¯•è®°å½•)
      - [æµ‹è¯•ä»»åŠ¡](#æµ‹è¯•ä»»åŠ¡)



---

## ä»»åŠ¡1ï¼šå¤šé¡¹å¼å›å½’

### 1.ä»»åŠ¡æè¿°
å¤šé¡¹å¼å›å½’æ˜¯ä¸€ç§å›å½’åˆ†æå½¢å¼ï¼Œåœ¨è¿™ç§å½¢å¼ä¸­ï¼Œè‡ªå˜é‡ ( x ) å’Œå› å˜é‡ ( y ) ä¹‹é—´çš„å…³ç³»è¢«å»ºæ¨¡ä¸º ( n ) é˜¶å¤šé¡¹å¼ã€‚ä½¿ç”¨æœºå™¨å­¦ä¹ çš„æ–¹æ³•æ¥åˆ›å»ºä¸€ä¸ªå¤šé¡¹å¼å›å½’æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¯ä»¥æ ¹æ®ç»™å®šçš„æ•°æ®é›†é¢„æµ‹ç»“æœã€‚æ•°æ®é›†ç”±è‡ªå˜é‡ ( x ) å’Œå› å˜é‡ ( y ) ç»„æˆï¼Œä½ çš„ä»»åŠ¡æ˜¯æ‰¾åˆ°ä¸€ä¸ªå¤šé¡¹å¼ï¼Œèƒ½æœ€å¥½åœ°æè¿° ( x ) å’Œ ( y ) ä¹‹é—´çš„å…³ç³»ã€‚

### 2.æ•°æ®é›†æè¿°
æœ¬å®éªŒé€‰å–æ•°æ®é›†åŒ…å«125ä¸ªæ ·æœ¬ç‚¹ï¼Œæ¯ä¸ªæ ·æœ¬å…·æœ‰ä¸€ä¸ªè‡ªå˜é‡( x )å’Œä¸€ä¸ªå› å˜é‡( y )ã€‚æ•°æ®é›†æ ¹æ®4:1çš„æ¯”ä¾‹åˆ’åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ã€‚

### 3.ä»»åŠ¡åˆ†æ
å¤šé¡¹å¼å›å½’æ˜¯ä¸€ç§å›å½’åˆ†ææ–¹å¼ï¼Œé€šè¿‡å¤šé¡¹å¼å‡½æ•°æ¥æ‹Ÿåˆæ•°æ®é›†ï¼Œè¿™ç§æ–¹æ³•é€‚ç”¨äºæ•°æ®å’Œä¸€ä¸ªæˆ–å¤šä¸ªéçº¿æ€§å…³ç³»çš„æƒ…å†µã€‚å¤šé¡¹å¼å›å½’æ¨¡å‹çš„å½¢å¼å¦‚ä¸‹ï¼š
<div align=center>

$$
y = \beta_0+\beta_1x+\beta_2x^2+\beta_3x^3+...+\beta_nx^n+\epsilon 
$$

</div>

åœ¨å¤šé¡¹å¼å›å½’æ¨¡å‹ä¸­ï¼Œè‡ªå˜é‡çš„ä¸åŒæ¬¡æ–¹$x^2$,$x^3$,...$x^n$è¢«ä½œä¸ºæ–°ç‰¹å¾åŠ å…¥åˆ°æ¨¡å‹ä¸­ã€‚é€šè¿‡è®­ç»ƒæ•°æ®ï¼Œä½¿ç”¨æ™®é€šæœ€å°äºŒä¹˜æ³•ç­‰ç®—æ³•æ¥ä¼°è®¡å›å½’ç³»æ•°$\{ \beta_0,\beta_1,\beta_2...\beta_n\}$,ä»è€Œæ•æ‰æ•°æ®ä¸­çš„éçº¿æ€§å…³ç³»ã€‚

#### æ™®é€šæœ€å°äºŒä¹˜æ³•
æ™®é€šæœ€å°äºŒä¹˜æ³•æ˜¯ä¼°è®¡å¤šé¡¹å¼å›å½’æ¨¡å‹ç³»æ•°çš„å¸¸ç”¨æ–¹æ³•ï¼Œå…¶ç›®æ ‡æ˜¯æœ€å°åŒ–å®é™…å€¼ä¸é¢„æµ‹å€¼ä¹‹é—´çš„è¯¯å·®å¹³æ–¹å’Œï¼š
<div align=center>

$RSS = \sum_{i=1}^{m} (y_i - \hat{y}_i)^2$

mä¸ºæ ·æœ¬æ•°ï¼Œ$y_i$ ä¸ºå®é™…å€¼, $\hat{y}_i$ä¸ºé¢„æµ‹å€¼ã€‚é€šè¿‡æœ€å°åŒ–RSSï¼Œå°±å¯ä»¥æ‰¾åˆ°æœ€ä½³çš„å›å½’ç³»æ•°$\beta_0,\beta_1,\beta_2,...,\beta_n$ã€‚

</div>

### 4.ä»£ç å®ç°ä¸åˆ†æ
#### 4.1 æ•°æ®é›†è¯»å–ä¸æ•°æ®é¢„å¤„ç†

æ•°æ®é›†å­˜å‚¨åœ¨`homwework.csv`æ–‡ä»¶ä¸­ï¼Œä½¿ç”¨`numpy`åº“çš„`loadtxt`æ–¹æ³•æ¥è¯»å–æ•°æ®ã€‚ä¹‹ååˆ©ç”¨`reshape`å¢åŠ ä¸€ä¸ªç»´åº¦ä»¥æ–¹ä¾¿æ¨¡å‹è®­ç»ƒã€‚

```python
import numpy as np
from sklearn.preprocessing import PolynomialFeatures
import pandas as pd

train_data = np.loadtxt(
    open("ä½œä¸šä¸€/poly_reg/train_dataset.csv", "rb"), delimiter=",", skiprows=1
)
test_data = np.loadtxt(
    open("ä½œä¸šä¸€/poly_reg/test_dataset.csv", "rb"), delimiter=",", skiprows=1
)
new_test_data = pd.read_csv('ä½œä¸šä¸€/new_test.csv',header=0)


x_train = train_data[:, 0].reshape(-1, 1)
y_train = train_data[:, 1].reshape(-1, 1)
x_test = test_data[:, 0].reshape(-1, 1)
y_test = test_data[:, 1].reshape(-1, 1)
new_x_test=np.array(new_test_data['x'].values).reshape(-1,1)
```
æ¥ç€è®¾ç½®å¤šé¡¹å¼é˜¶æ•°ï¼Œå¹¶é‡‡ç”¨`sklearn.preprocessing`ä¸­çš„`PolynomialFeatures`æ–¹æ³•å¯¹è‡ªå˜é‡è¿›è¡Œç‰¹å¾æ‹“å±•ã€‚
```python
degree = 4

# å¤šé¡¹å¼ç‰¹å¾
poly_features = PolynomialFeatures(degree=degree)
x_train_poly = poly_features.fit_transform(x_train)
x_test_poly = poly_features.transform(x_test)
new_x_test_poly=poly_features.transform(new_x_test)
```

#### 4.2 æ¨¡å‹è®­ç»ƒ
é‡‡ç”¨`sklearn.linear_model`ä¸­çš„`LinearRegression`ä½œä¸ºæ¨¡å‹ï¼Œé€šè¿‡æ¨¡å‹çš„`fit()`æ–¹æ³•è¿›è¡Œè®­ç»ƒ

```python
# æ¨¡å‹è®­ç»ƒ
from sklearn.linear_model import LinearRegression

model = LinearRegression()
model.fit(x_train_poly, y_train)

```
#### 4.3 æ¨¡å‹è¯„ä»·ä¸é¢„æµ‹
ä½¿ç”¨æµ‹è¯•é›†å¯¹æ¨¡å‹è¿›è¡Œè¯„ä»·ï¼Œå¹¶å¯¹ä½œä¸šæ•°æ®é›†è¿›è¡Œé¢„æµ‹ï¼Œé€šè¿‡`sklearn.metrics`ä¸­çš„`mean_squared_error`è®¡ç®—æµ‹è¯•é›†çš„MSEï¼Œå°†ä½œä¸šæ•°æ®é›†çš„é¢„æµ‹ç»“æœç›´æ¥å­˜å‚¨å›åŸæ–‡ä»¶
```python
from sklearn.metrics import mean_squared_error

#æµ‹è¯•
train_predictions = model.predict(x_train_poly)
test_predictions = model.predict(x_test_poly)

#æ–¹å·®
train_mse= mean_squared_error(y_train,train_predictions)
test_mse = mean_squared_error(y_test,test_predictions)

print("è®­ç»ƒé›†æ–¹å·®:",train_mse)
print("æµ‹è¯•é›†æ–¹å·®",test_mse)

#ä½œä¸šæäº¤éƒ¨åˆ†
new_test_prediction=model.predict(new_x_test_poly)
#print(new_test_prediction)
new_test_data['y']=new_test_prediction
new_test_data.to_csv("ä½œä¸šä¸€/homework.csv",index=False)
```

#### 4.4 ç»“æœå¯è§†åŒ–
ä¸ºäº†èƒ½å¤Ÿæ›´åŠ ç›´è§‚çš„è§‚å¯Ÿæ¨¡å‹è®­ç»ƒæ•ˆæœï¼Œé‡‡ç”¨`matplotlib.pyplot`è¿›è¡Œå›¾åƒç»˜åˆ¶ï¼Œå¹¶ä¸”é€šè¿‡æ ¼å¼åŒ–è¾“å‡ºæ¥è¾“å‡ºæœ€ç»ˆçš„å›å½’æ›²çº¿è¡¨è¾¾å¼ã€‚
```python
import matplotlib.pyplot as plt

# ç»˜å›¾
plt.scatter(x_train, y_train, color='blue', label='Train data')
plt.scatter(x_test, y_test, color='red', label='Test data')

# ä¸ºäº†ç»˜åˆ¶å¹³æ»‘æ›²çº¿ï¼Œç”Ÿæˆè®¸å¤šç‚¹æ¥é¢„æµ‹
x_range = np.linspace(x_train.min(), x_train.max(), 500).reshape(-1, 1)
x_range_poly = poly_features.transform(x_range)
y_range_pred = model.predict(x_range_poly)

# è¾“å‡ºå¤šé¡¹å¼è¡¨è¾¾å¼ï¼ˆä»…åŒ…å«éé›¶ç³»æ•°çš„é¡¹ï¼‰
coefficients = model.coef_[0]
intercept = model.intercept_[0]

# æ„å»ºå¤šé¡¹å¼è¡¨è¾¾å¼å­—ç¬¦ä¸²
poly_expression = f"f(x) = {intercept:.2f}"
for i in range(1, degree + 1):
    if coefficients[i] != 0:
        poly_expression += f" + {coefficients[i]:.2f}x^{i}"

print("å¤šé¡¹å¼å›å½’æ¨¡å‹è¡¨è¾¾å¼ï¼š", poly_expression)

#ç»˜å›¾
plt.plot(x_range, y_range_pred, color='green', label=f'Polynomial Degree {degree}')
plt.title(poly_expression)
plt.xlabel('X')
plt.ylabel('Y')
plt.legend()
plt.savefig("ä½œä¸šä¸€/png_homework.png")
plt.show()
```


### 5.æµ‹è¯•ç»“æœå±•ç¤º
#### 5.1 ä¸åŒé˜¶æ•°ä¸‹çš„MSE
degree=2  

![alt text](image.png)  
degree=4  

![alt text](image-1.png)  
degree=6  

![alt text](image-2.png)  
degree=8  

![alt text](image-3.png)  

#### 5.2 ä¸åŒé˜¶æ•°ä¸‹çš„å›¾åƒæ›²çº¿ä»¥åŠè¡¨è¾¾å¼
degree=2
![alt text](img_1.png)
degree=4
![alt text](img_2.png)
degree=6
![alt text](img_3.png)
degree=8
![alt text](img_4.png)

---

## ä»»åŠ¡2ï¼šæ¦‚ç‡åˆ†ç±»æ³•
### 1.ä»»åŠ¡æè¿°
ä½¿ç”¨è´å¶æ–¯ä¼°è®¡æˆ–MLEï¼ˆæœ€å¤§ä¼¼ç„¶ä¼°è®¡ï¼‰ï¼Œæ¥é¢„æµ‹é¸¢å°¾èŠ±æ•°æ®é›†ä¸­èŠ±çš„ç§ç±»ã€‚
### 2.æ•°æ®é›†æè¿°
é¸¢å°¾èŠ±æ•°æ®é›†æ˜¯ç»Ÿè®¡å­¦å’Œæœºå™¨å­¦ä¹ ä¸­ç”¨äºåˆ†ç±»çš„ç»å…¸æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†åŒ…å«äº†ä¸‰ç§ä¸åŒçš„é¸¢å°¾èŠ±ï¼šSetosaã€Versicolorå’ŒVirginicaï¼Œæ¯ç§å„50ä¸ªæ ·æœ¬ã€‚æ¯ä¸ªæ ·æœ¬æœ‰å››ä¸ªå±æ€§ï¼šè¼ç‰‡é•¿åº¦ã€è¼ç‰‡å®½åº¦ã€èŠ±ç“£é•¿åº¦å’ŒèŠ±ç“£å®½åº¦ï¼Œæ‰€æœ‰çš„æµ‹é‡å•ä½éƒ½æ˜¯å˜ç±³ã€‚æ•°æ®é›†æ ¹æ®4:1çš„æ¯”ä¾‹åˆ’åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ã€‚æ¦‚ç‡åˆ†ç±»æ³•æ˜¯ä¸€ç§åŸºäºæ¦‚ç‡ç†è®ºçš„æ–¹æ³•ï¼Œé€‚åˆå¤„ç†æ­¤ç±»åˆ†ç±»é—®é¢˜ã€‚
### 3.ä»»åŠ¡åˆ†æ
è´å¶æ–¯ä¼°è®¡å’ŒMLEæ˜¯ç»Ÿè®¡å­¦ä¸­å¸¸ç”¨çš„ä¸¤ç§æ–¹æ³•ï¼Œç”¨äºå‚æ•°ä¼°è®¡ä¸åˆ†ç±»ä»»åŠ¡ã€‚å…¶ä¸­ï¼Œè´å¶æ–¯ä¼°è®¡åˆ©ç”¨å…ˆéªŒçŸ¥è¯†å’Œæ•°æ®æ¥è¿›è¡Œå‚æ•°ä¼°è®¡ï¼ŒMLEåˆ™æ˜¯é€šè¿‡æœ€å¤§åŒ–ä¼¼ç„¶å‚æ•°æ¥ä¼°è®¡å‚æ•°ã€‚è¿™ä¸¤ç§æ–¹æ³•éƒ½é€‚ç”¨äºåˆ†ç±»é—®é¢˜ï¼Œå¦‚æœ¬æ¬¡çš„é¸¢å°¾èŠ±æ•°æ®é›†åˆ†ç±»ã€‚

åœ¨æœ¬æ¬¡ä»»åŠ¡ä¸­ï¼Œæ•°æ®é›†å·²ç»æå‰å¤„ç†å¥½ï¼Œç•™ä¸‹äº†éœ€è¦çš„ç‰¹å¾å¹¶å­˜å‚¨äºcsvæ–‡ä»¶ä¸­ï¼Œåªéœ€è¦è¯»å–å¹¶åšç®€å•çš„é¢„å¤„ç†å³å¯ã€‚ä¹‹åè‡ªè¡Œå®ç°ä¸€ä¸ªé«˜æ–¯è´å¶æ–¯åˆ†ç±»å™¨ï¼Œè®­ç»ƒå¹¶è¯„ä¼°å³å¯
#### æ•°å­¦åŸºç¡€
##### è´å¶æ–¯ä¼°è®¡
è´å¶æ–¯ä¼°è®¡é€šè¿‡ç»“åˆå…ˆéªŒåˆ†å¸ƒå’Œä¼¼ç„¶å‡½æ•°æ¥ä¼°è®¡å‚æ•°ã€‚è´å¶æ–¯å…¬å¼å¦‚ä¸‹ï¼š
<div align = center>

$P(\theta|X)=\frac{P(X|\theta)P(\theta)}{P(X)}$

</div>  

å…¶ä¸­ï¼Œ$P(\theta|X)$ æ˜¯ç»™å®šæ•°æ® $X$ åå‚æ•° $\theta$ çš„åéªŒæ¦‚ç‡ï¼›$P(X|\theta)$ æ˜¯ç»™å®šå‚æ•° $\theta$ åæ•°æ® $X$ çš„ä¼¼ç„¶å‡½æ•°ï¼›$P(\theta)$ æ˜¯å‚æ•° $\theta$ çš„æ¦‚ç‡ï¼›$P(X)$ æ˜¯æ•°æ®Xçš„æ¦‚ç‡
##### æœ€å¤§ä¼¼ç„¶ä¼°è®¡
æœ€å¤§ä¼¼ç„¶ä¼°è®¡é€šè¿‡æœ€å¤§åŒ–ä¼¼ç„¶å‡½æ•°æ¥ä¼°è®¡å‚æ•°ã€‚ä¼¼ç„¶å‡½æ•°è¡¨ç¤ºç»™å®šå‚æ•° $\theta$ æ—¶æ•°æ®å‡ºç°çš„æ¦‚ç‡ã€‚æœ€å¤§ä¼¼ç„¶ä¼°è®¡å°±æ˜¯è¦æ‰¾åˆ°ä½¿å¾—ä¼¼ç„¶å‡½æ•°æœ€å¤§çš„å‚æ•° $\theta$. ä¼¼ç„¶å‡½æ•°å¦‚ä¸‹
<div align=center>

$ ğ¿(ğœƒ|ğ‘‹)=ğ‘ƒ(ğ‘‹|ğœƒ)$

</div>

### 4.ä»£ç å®ç°ä¸åˆ†æ
#### 4.1 æ•°æ®é›†è¯»å–ä¸åˆ’åˆ†
ä½¿ç”¨`sklearn.datasets`ä¸­çš„`load_iris()`åŠ è½½é¸¢å°¾èŠ±æ•°æ®é›†ï¼Œå°†ç‰¹å¾æ•°æ®`X`å’Œç›®æ ‡æ ‡ç­¾`Y`åˆ†ç¦»ï¼Œå¹¶åˆ©ç”¨`sklearn.model_selection`ä¸­çš„`train_test_split`è¿›è¡Œè®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„åˆ’åˆ†ã€‚

```python
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split

iris = load_iris()
X = iris.data
Y = iris.target

# åˆ’åˆ†æ•°æ®é›†
x_train, x_val, y_train, y_val = train_test_split(
    X, Y, test_size=0.2, random_state=1919
)

```

#### 4.2 å®šä¹‰é«˜æ–¯æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨ç±»
å®šä¹‰ä¸€ä¸ªé«˜æ–¯æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨ä½œä¸ºæ¨¡å‹ï¼Œå…¶ä¸­`fit()`æ–¹æ³•è¿›è¡Œäº†æ¯ä¸ªç±»å‡å€¼ï¼Œæ–¹å·®ï¼Œå…ˆéªŒæ¦‚ç‡çš„è®¡ç®—ä»¥ä½œä¸ºé¢„æµ‹ä¾æ®ã€‚`_calculate_likehlihood()`æ–¹æ³•ä¸­è®¡ç®—äº†ç±»çš„ç‰¹å¾çš„ä¼¼ç„¶ï¼Œå‡è®¾å®ƒç¬¦åˆé«˜æ–¯åˆ†å¸ƒï¼Œ`_calculate_posterior()`æ–¹æ³•å¯¹è¾“å…¥æ•°æ®è¿›è¡ŒåéªŒæ¦‚ç‡çš„è®¡ç®—ï¼Œå¹¶ä¸”è¿”å›æ¦‚ç‡æœ€é«˜çš„é¢„æµ‹ç±»ã€‚æœ€åå®šä¹‰äº†`predict()`æ–¹æ³•æ¥æ”¶æ•°æ®å¹¶è¿”å›é¢„æµ‹ç»“æœã€‚

```python
import numpy as np

class GaussianNaiveBayes:
    def fit(self, x, y):
        # æ‰€æœ‰target
        self.classes = np.unique(y)

        # åˆå§‹åŒ–å‡å€¼ï¼Œæ–¹å·®ï¼Œå…ˆéªŒæ¦‚ç‡
        self.mean = np.zeros((len(self.classes), x.shape[1]), dtype=np.float64)
        self.var = np.zeros((len(self.classes), x.shape[1]), dtype=np.float64)
        self.priors = np.zeros(len(self.classes), dtype=np.float64)

        # è®¡ç®—æ¯ä¸ªç±»çš„å‡å€¼ï¼Œæ–¹å·®å’Œå…ˆéªŒæ¦‚ç‡
        for idx, c in enumerate(self.classes):
            x_c = x[y == c]
            self.mean[idx, :] = x_c.mean(axis=0)
            self.var[idx, :] = x_c.var(axis=0)
            self.priors[idx] = x_c.shape[0] / float(x.shape[0])

    def _calculate_likehlihood(self, class_idx, x):
        # è®¡ç®—ç»™å®šç±»ä¸‹ç‰¹å¾çš„ä¼¼ç„¶ï¼ˆå‡è®¾ä¼¼ç„¶æœä»é«˜æ–¯åˆ†å¸ƒ)
        mean = self.mean[class_idx]
        var = self.mean[class_idx]
        numerator = np.exp(-((x - mean) ** 2) / (2 * var))  # é«˜æ–¯åˆ†å¸ƒçš„åˆ†å­éƒ¨åˆ†
        denominator = np.sqrt(2 * np.pi * var)  # åˆ†æ¯éƒ¨åˆ†
        return numerator / denominator

    def _calculate_posterior(self, x):
        # è®¡ç®—åéªŒæ¦‚ç‡ï¼Œå¹¶è¿”å›å…·æœ‰æœ€é«˜åéªŒæ¦‚ç‡çš„ç±»
        posteriors = []
        for idx, c in enumerate(self.classes):
            piror = np.log(self.priors[idx])  # å…ˆéªŒæ¦‚ç‡å–å¯¹æ•°
            conditional = np.sum(
                np.log(self._calculate_likehlihood(idx, x))
            )  # æ¡ä»¶æ¦‚ç‡å–å¯¹æ•°å¹¶æ±‚å’Œ
            posterior = piror + conditional  # è®¡ç®—åéªŒæ¦‚ç‡
            posteriors.append(posterior)
        return self.classes[np.argmax(posteriors)]  # è¿”å›åéªŒæ¦‚ç‡æœ€é«˜çš„ç±»

    def predict(self, X):
        y_pred = [self._calculate_posterior(x) for x in X]
        return np.array(y_pred)
```

#### 4.3 è®­ç»ƒæ¨¡å‹å¹¶è¯„ä¼°
å¯¹è‡ªå®šä¹‰çš„é«˜æ–¯æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨è¿›è¡Œè®­ç»ƒï¼Œå¹¶ç”¨æµ‹è¯•é›†æ•°æ®å¯¹å…¶è¿›è¡Œè¯„ä¼°ï¼Œé‡‡ç”¨`sklearn.metrics`ä¸­çš„`accuracy_score`æ–¹æ³•è®¡ç®—è¯„ä¼°çš„accuracyã€‚

```python
from sklearn.metrics import accuracy_score

model = GaussianNaiveBayes()
model.fit(x_train,y_train)
y_pred = model.predict(x_val)
accuracy=accuracy_score(y_val,y_pred)
print(f"æ¨¡å‹å‡†ç¡®ç‡:{accuracy}")

```

#### 4.4 æµ‹è¯•æ¨¡å‹å¹¶ä¿å­˜ç»“æœ
ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹å¯¹ä½œä¸šæ•°æ®è¿›è¡Œé¢„æµ‹ï¼Œå¹¶ä¿å­˜é¢„æµ‹ç»“æœåˆ°åŸæ–‡ä»¶ä¸­
```python
import pandas as pd

# æµ‹è¯•ç¯èŠ‚
test_iris = pd.read_csv("hmwk2/iris_test.csv", header=0)

test_data = np.array(test_iris[test_iris.columns[:4]].values).reshape(-1, 4)
# print(test_data)
pred_test_target = model.predict(test_data)
#print(pred_test_target)
test_iris["species"]=pred_test_target

test_iris.to_csv("hmwk2/homework.csv",index=False)
```

### 5.è¿è¡Œç»“æœå±•ç¤º
æœ€ç»ˆè¯„ä¼°çš„æ­£ç¡®ç‡ä¸º`0.96`ï¼Œæ ¹æ®åŠ©æ•™åé¦ˆä¿¡æ¯ï¼Œå¯¹ä½œä¸šé›†çš„æ­£ç¡®ç‡ä¸º`0.867`
![alt text](image-4.png)

---

## ä»»åŠ¡3ï¼šæ”¯æŒå‘é‡æœº
### 1.ä»»åŠ¡æè¿°
åŸºäºSVMæ„å»ºæœºå™¨å­¦ä¹ æ¨¡å‹, å¯¹Omniglotä¸­çš„å­—ç¬¦åˆ†ç±»
### 2.æ•°æ®é›†æè¿°
Omniglot Datasetæ˜¯å…¨è¯­è¨€æ–‡å­—æ•°æ®é›†ï¼ŒåŒ…å«å„ç§è¯­è¨€çš„ä¸åŒå­—æ¯è¡¨ï¼Œå¦‚æ—¥è¯­çš„å¹³å‡åå’Œç‰‡å‡åï¼ŒéŸ©è¯­çš„å…ƒéŸ³å’Œè¾…éŸ³ï¼Œæœ€å¸¸è§çš„æ‹‰ä¸å­—æ¯abcdç­‰ã€‚Omniglot Datasetå…±åŒ…å«50ä¸ªä¸åŒè¯­è¨€çš„å­—æ¯è¡¨ï¼Œæ¯ä¸ªå­—æ¯è¡¨ä¸­åŒ…å«ä¸åŒçš„å­—ç¬¦ï¼Œå…±1623ç§å­—ç¬¦ï¼Œæ¯ç§å­—ç¬¦ç”±20ä¸ªä¸åŒçš„äººä¹¦å†™ã€‚æœ¬æ¬¡å®éªŒé€‰å–å…¶ä¸­200ç§å­—ç¬¦ï¼Œæ¯ç§å­—ç¬¦çš„15å¼ å›¾ç‰‡ä½œä¸ºè®­ç»ƒé›†ï¼Œ5å¼ å›¾ç‰‡ä½œä¸ºæµ‹è¯•é›†ã€‚
### 3.ä»»åŠ¡åˆ†æ
æœ¬æ¬¡ä»»åŠ¡è¦æ±‚ä½¿ç”¨SVMå¯¹Omniglotæ•°æ®é›†ä¸­çš„å­—ç¬¦è¿›è¡Œåˆ†ç±»ã€‚
#### SVM
SVMï¼Œä¹Ÿå°±æ˜¯æ”¯æŒå‘é‡æœºï¼Œæ˜¯ä¸€ç§ç›‘ç£å­¦ä¹ æ¨¡å‹ï¼Œç”¨äºåˆ†ç±»å’Œå›å½’åˆ†æã€‚SVMçš„ç›®æ ‡æ˜¯æ‰¾åˆ°ä¸€ä¸ªèƒ½å¤Ÿæœ€å¥½åˆ†ç¦»ä¸åŒç±»åˆ«çš„è¶…å¹³é¢ã€‚
æ•°æ®ä¸­æœ€é‡è¦çš„éƒ¨åˆ†æ˜¯æ”¯æŒå‘é‡ï¼Œä¹Ÿå°±æ˜¯è·ç¦»ç›®æ ‡è¶…å¹³é¢æœ€è¿‘çš„é‚£äº›æ•°æ®ç‚¹ã€‚è¿™äº›æ•°æ®å¯¹ç¡®å®šè¶…å¹³é¢æœ‰ç€å†³å®šæ€§çš„ä½œç”¨ã€‚
SVMä¸ä»…è¦æ‰¾åˆ°ä¸€ä¸ªè¶…å¹³é¢ï¼Œè¿˜è¦ä½¿è¿™ä¸ªè¶…å¹³é¢ä¸æ”¯æŒå‘é‡çš„é—´éš”æœ€å¤§åŒ–ä»¥æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚
å½“æ•°æ®æ˜¯çº¿æ€§å¯åˆ†çš„ï¼Œä¹Ÿå°±æ˜¯å¯ä»¥é€šè¿‡ä¸€ä¸ªçº¿æ€§è¶…å¹³é¢å®Œå…¨åˆ†å¼€ï¼Œé‚£ä¹ˆSVMå¯ä»¥ç›´æ¥æ‰¾åˆ°ä¸€ä¸ªåˆé€‚çš„è¶…å¹³é¢ã€‚ä½†æ˜¯å¦‚æœæ•°æ®çº¿æ€§ä¸å¯åˆ†ï¼Œå°±è¦é€šè¿‡æ ¸`kernel`æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚åœ¨æœ¬æ¬¡ä»»åŠ¡ä¸­è®¡åˆ’ä½¿ç”¨RBFæ ¸çŸ©é˜µï¼Œä¹Ÿå°±æ˜¯é«˜æ–¯æ ¸ï¼Œå®ƒå…·æœ‰è¾ƒå¼ºçš„çµæ´»æ€§å’Œé€‚åº”æ€§ï¼Œå¸¸ç”¨äºéçº¿æ€§æ•°æ®ã€‚å…¶å…¬å¼å¦‚ä¸‹
<div align=center>

$K(x,x')=\exp(-\gamma\|x-x'\|^2)$

</div>

$x$å’Œ$x'$æ˜¯ä¸¤ä¸ªæ ·æœ¬ç‚¹ï¼Œ$\|x-x'\|$æ˜¯ä¸¤ä¸ªæ ·æœ¬é—´çš„æ¬§å‡ é‡Œå¾—è·ç¦»ï¼Œ$\gamma$æ˜¯ç”¨äºæ§åˆ¶æ ¸å‡½æ•°å®½åº¦çš„è¶…å‚æ•°
åœ¨æœ¬æ¬¡ä»»åŠ¡ä¸­ï¼Œå…ˆè¿›è¡Œæ•°æ®è¯»å–ä¸é¢„å¤„ç†ï¼Œä¹‹åè‡ªå®šä¹‰ä¸€ä¸ªSVMï¼Œç”±äºæœ¬æ¬¡æ˜¯å¤šç±»åˆ†ç±»ä»»åŠ¡ï¼Œè€ŒSVMæœ¬è´¨æ˜¯ä¸€ä¸ªäºŒåˆ†ç±»æ¨¡å‹ï¼Œå› æ­¤æœ¬æ¬¡å‡†å¤‡é‡‡ç”¨â€œä¸€å¯¹ä¸€â€ç­–ç•¥ä»¥ä½¿SVMå¯ä»¥è§£å†³å¤šåˆ†ç±»é—®é¢˜ã€‚
æ‰€è°“â€œä¸€å¯¹ä¸€â€æ˜¯æŒ‡å¯¹äºæ¯ä¸¤ä¸ªä¸åŒçš„ç±»åˆ«éƒ½è®­ç»ƒä¸€ä¸ªSVMåˆ†ç±»å™¨ï¼Œæœ€ç»ˆé¢„æµ‹ç±»åˆ«ç”±æ‰€æœ‰çš„åˆ†ç±»å™¨æŠ•ç¥¨å†³å®šã€‚è¿™ç§æ–¹å¼æ›´åŠ å‡†ç¡®ï¼Œä½†æ˜¯è®¡ç®—å¼€é”€æ¯”è¾ƒå¤§
### 4.ä»£ç å®ç°ä¸åˆ†æ
#### 4.1 æ•°æ®è¯»å–ï¼Œé¢„å¤„ç†ä¸æ•°æ®é›†åˆ’åˆ†
é€šè¿‡`os.path`ä¸­çš„å¤šä¸ªæ–¹æ³•ç¡®å®šæ•°æ®æ‰€åœ¨çš„è·¯å¾„ä»¥ä¿è¯åœ¨ä¸åŒç¯å¢ƒä¸‹ç¨‹åºéƒ½å¯ä»¥æ­£å¸¸è¯»å–æ•°æ®ã€‚ç”±äºæ•°æ®å­˜å‚¨åœ¨matæ–‡ä»¶ä¸­ï¼Œæ•…ä½¿ç”¨`scipy.io`ä¸­çš„`loadmat`æ–¹æ³•è¿›è¡Œæ•°æ®åŠ è½½ã€‚
é¢„å¤„ç†ç¯èŠ‚ä¸­ï¼Œå…ˆå°†æ•°æ®ä»28*28å±•å¹³ä¸ºé•¿784çš„ä¸€ç»´å‘é‡ï¼Œä¹‹åæ ¹æ®æ•°æ®é›†ä»‹ç»ï¼Œæ‰‹åŠ¨åˆ›å»ºæ ‡ç­¾1~200ï¼Œæ¯ä¸ªæ ‡ç­¾åŒ…å«15ä¸ªæ ·æœ¬ã€‚æœ€åå€ŸåŠ©`sklearn.preprocessing`ä¸­çš„`MinMaxScaler`æ–¹æ³•è¿›è¡Œæ•°æ®æ ‡å‡†åŒ–ä»¥å¢å¼ºæ¨¡å‹è®­ç»ƒæ•ˆæœã€‚æœ€åé‡‡ç”¨`sklearn.model_selection`ä¸­çš„`train_test_split`æ–¹æ³•è¿›è¡Œæ•°æ®é›†åˆ’åˆ†ã€‚

```python
import scipy.io as sio
import numpy as np
import os
import pandas as pd
from  import train_test_split
from sklearn.preprocessing import MinMaxScaler

# åŠ è½½æ•°æ®
current_dir = os.path.dirname(os.path.abspath(__file__))
train_data_path = os.path.join(current_dir, "train_data.mat")
test_data_path = os.path.join(current_dir, "test_data.mat")

train_data = sio.loadmat(train_data_path)
test_data = sio.loadmat(test_data_path)

# å°†è®­ç»ƒæ•°æ®å±•å¼€ä¸º 28*28 = 784 çš„ä¸€ç»´å‘é‡
x_train = train_data["train"].reshape(-1, 28 * 28)
# åˆ›å»ºæ ‡ç­¾ 1-200, æ¯ä¸ªæ ‡ç­¾15ä¸ªæ ·æœ¬
y_train = np.repeat(np.arange(1, 201), 15)
# å°†æµ‹è¯•æ•°æ®å±•å¼€ä¸º 28*28 = 784 çš„ä¸€ç»´å‘é‡
x_test = test_data["test"].reshape(-1, 28 * 28)

# æ•°æ®æ ‡å‡†åŒ–
scaler = MinMaxScaler()
x_train = scaler.fit_transform(x_train)
x_test = scaler.transform(x_test)

# åˆ’åˆ†æ•°æ®é›†
x_train, x_val, y_train, y_val = train_test_split(
    x_train, y_train, test_size=0.25, stratify=y_train, random_state=1919
)
```
#### 4.2 SVMå®ç°
å®ç°ä¸€ä¸ªä¸€å¯¹ä¸€çš„SVMæ¨¡å‹ï¼Œæ ¸å‡½æ•°ä½¿ç”¨RBFã€‚
åœ¨`rbf_kernel`æ–¹æ³•ä¸­è¿›è¡ŒRBFæ ¸çŸ©é˜µçš„è®¡ç®—ã€‚
`objective`æ–¹æ³•è®¡ç®—äº†SVMçš„å¯¹å¶å½¢å¼ï¼Œç”¨äºåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é€šè¿‡ä¼˜åŒ–æ‹‰æ ¼æœ—æ—¥ä¹˜å­$\alpha$ä»¥æ‰¾åˆ°æœ€ä¼˜çš„åˆ†ç±»å†³ç­–è¾¹ç•Œï¼Œå…¶å…¬å¼ä¸º

<div align=center>

$ L(\alpha)=\sum_{i=1}^N\alpha_i-\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_jK(x_i,x_j)$

</div>

`zerofun`æ–¹æ³•å®šä¹‰äº†çº¦æŸæ¡ä»¶ï¼Œä½¿å¾—æ‹‰æ ¼æœ—æ—¥ä¹˜å­çš„åŠ æƒå’Œä¸º0ã€‚`fit`æ–¹æ³•ä½¿ç”¨äºŒåˆ†ç±»SVMé‡‡ç”¨ä¸€å¯¹ä¸€ç­–ç•¥è®­ç»ƒæ¯å¯¹ç±»çš„åˆ†ç±»å™¨ã€‚`predict`æ–¹æ³•ç”¨äºé¢„æµ‹æ–°çš„æ•°æ®ç‚¹çš„ç±»åˆ«ï¼Œé€šè¿‡æŠ•ç¥¨æœºåˆ¶ç¡®å®šæœ€ç»ˆåˆ†ç±»ç»“æœ
```python
from scipy.optimize import minimize
import itertools
# å®ç°SVM
class NonlinearSVM:
    def __init__(self, C=1.0, gamma=0.05):
        self.C = C# æ­£åˆ™åŒ–å‚æ•°
        self.gamma = gamma# RBFæ ¸å‚æ•°
        self.classifiers = {}# å­˜å‚¨æ¯å¯¹ç±»çš„åˆ†ç±»å™¨

    def rbf_kernel(self, X1, X2):#è®¡ç®—RBFæ ¸çŸ©é˜µ
        K = np.exp(-self.gamma * np.linalg.norm(X1[:, np.newaxis] - X2, axis=2) ** 2)
        return K

    def objective(self, alpha, y, K):  # SVMå¯¹å¶çš„ç›®æ ‡å‡½æ•°ã€‚
        return 0.5 * np.sum(
            alpha * alpha[:, np.newaxis] * y * y[:, np.newaxis] * K
        ) - np.sum(alpha)

    def zerofun(self, alpha, y):  # çº¦æŸæ¡ä»¶
        return np.dot(alpha, y)

    def fit(self, X, y):
        classes = np.unique(y)
        for i, j in itertools.combinations(classes, 2):# å¯¹æ¯å¯¹ç±»åˆ«è®­ç»ƒä¸€ä¸ªåˆ†ç±»å™¨
            print(f"Training for classes {i} vs {j}")
            idx = np.where((y == i) | (y == j))[0]# è·å–å±äºè¿™ä¸¤ä¸ªç±»åˆ«çš„æ ·æœ¬
            X_ij, y_ij = X[idx], y[idx]
            y_ij = np.where(y_ij == i, 1, -1)# å°†æ ‡ç­¾è½¬æ¢ä¸º+1å’Œ-1

            K = self.rbf_kernel(X_ij, X_ij)# è®¡ç®—RBFæ ¸çŸ©é˜µ

            N = len(y_ij)
            alpha0 = np.zeros(N)# åˆå§‹åŒ–æ‹‰æ ¼æœ—æ—¥ä¹˜å­
            B = [(0, self.C) for _ in range(N)]# å®šä¹‰alphaçš„è¾¹ç•Œ
            constraints = {"type": "eq", "fun": lambda alpha: self.zerofun(alpha, y_ij)}# å®šä¹‰çº¦æŸæ¡ä»¶
            
            # æœ€å°åŒ–ç›®æ ‡å‡½æ•°
            res = minimize(
                self.objective,
                alpha0,
                args=(y_ij, K),
                bounds=B,
                constraints=constraints,
            )
            alpha = res.x

            sv_idx = np.where(alpha > 1e-5)[0]
            b = np.mean([y_ij[k] - np.sum(alpha * y_ij * K[k]) for k in sv_idx])

            self.classifiers[(i, j)] = (alpha, b, X_ij, y_ij)

    def predict(self, X):
        votes = np.zeros((len(X), len(self.classifiers)))
        for k, ((i, j), (alpha, b, X_train, y_train)) in enumerate(
            self.classifiers.items()
        ):
            print(f"Test for classes {i} vs {j}")
            K = self.rbf_kernel(X_train, X)
            predictions = K.T @ (alpha * y_train) + b
            votes[:, k] = np.where(predictions > 0, i, j)
        y_pred = np.apply_along_axis(
            lambda x: np.bincount(x.astype(int)).argmax(), axis=1, arr=votes
        )
        return y_pred

```

#### 4.3 è®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹
ä½¿ç”¨è‡ªå®šä¹‰çš„SVMæ¨¡å‹çš„`fit`æ–¹æ³•è¿›è¡Œæ¨¡å‹è®­ç»ƒï¼Œå¹¶è®¡ç®—åœ¨æµ‹è¯•é›†ä¸Šçš„æ­£ç¡®ç‡
```python
from sklearn.metrics import accuracy_score

# è®­ç»ƒSVMæ¨¡å‹
svm = NonlinearSVM()
print("å¼€å§‹è®­ç»ƒ")
svm.fit(x_train, y_train)

# è®¡ç®—å‡†ç¡®ç‡
print("è¿›è¡Œå‡†ç¡®ç‡è®¡ç®—")
y_val_pred = svm.predict(x_val)
accuracy = accuracy_score(y_val, y_val_pred)
print("å‡†ç¡®ç‡:", accuracy)

```

#### 4.4 ä½œä¸šç»“æœå½•å…¥
ä½¿ç”¨æ¨¡å‹çš„`predict`æ–¹æ³•å¯¹ä½œä¸šæ•°æ®é›†è¿›è¡Œé¢„æµ‹ï¼Œå¹¶å°†é¢„æµ‹ç»“æœå­˜å‚¨å›åŸæ–‡ä»¶ã€‚
```python
# ä½œä¸šéƒ¨åˆ†
print("è¿›è¡Œä½œä¸šç»“æœè®¡ç®—ä¸å½•å…¥")
y_test_pred = svm.predict(x_test)
hmwk_csv = pd.read_csv(os.path.join(current_dir, "submission.csv"), header=0)
hmwk_csv["é¢„æµ‹ç»“æœ"] = y_test_pred

hmwk_csv.to_csv(os.path.join(current_dir, "submission.csv"), index=False)
input()

```
### 5.è¿è¡Œç»“æœå±•ç¤º
![alt text](image-5.png)

---

## ä»»åŠ¡4ï¼šå†³ç­–æ ‘
### 1.ä»»åŠ¡æè¿°
åŸºäºå†³ç­–æ ‘æ„å»ºæœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œæ ¹æ®ä¹˜å®¢çš„ç‰¹å¾é¢„æµ‹å…¶åœ¨ Titanic æ²‰èˆ¹äº‹ä»¶ä¸­æ˜¯å¦å¹¸å­˜ã€‚
### 2.æ•°æ®é›†æè¿°
Titanicæ•°æ®é›†ä¸­ä¹˜å®¢çš„ç‰¹å¾åŒ…å«å®¢èˆ±ç­‰çº§ã€æ€§åˆ«ã€å¹´é¾„ã€åœ¨ Titanic å·ä¸Šçš„åŒä¼´/é…å¶æ•°é‡ã€èˆ¹ç¥¨ç¼–å·ã€ç¥¨ä»·ç­‰ã€‚å¯¹äºæ¯ä¸€ä¸ªä¹˜å®¢éƒ½åŒ…å«äº†å…¶æ˜¯å¦åœ¨Titanicç¾éš¾ä¸­ç”Ÿè¿˜çš„ä¿¡æ¯ï¼ˆSurvivedï¼‰ï¼Œä½œä¸ºçœŸå®å€¼æ ‡ç­¾ã€‚
### 3.ä»»åŠ¡åˆ†æ

#### å†³ç­–æ ‘
å†³ç­–æ ‘æ˜¯ä¸€ç§ç›‘ç£å­¦ä¹ ç®—æ³•ï¼Œé€‚ç”¨äºåˆ†ç±»å’Œå›å½’ä»»åŠ¡ï¼Œå…¶ä¸»è¦æ€æƒ³å°±æ˜¯é€šè¿‡ä¸€ç³»åˆ—çš„ç­›é€‰æ¡ä»¶ï¼Œå°†æ•°æ®é›†åˆ†å‰²ä¸ºæ›´å°çš„å­é›†ï¼Œå½¢æˆä¸€ä¸ªç±»ä¼¼äºæ ‘çŠ¶çš„ç»“æ„ã€‚æ¯ä¸ªèŠ‚ç‚¹ä»£è¡¨ä¸€ä¸ªç‰¹å¾ï¼Œåˆ†æ”¯ä¸ºè¯¥ç‰¹å¾çš„å¯èƒ½å€¼ï¼Œå¶å­èŠ‚ç‚¹åˆ™ä»£è¡¨åˆ†ç±»ç»“æœæˆ–å›å½’å€¼ã€‚
æœ¬æ¬¡è€ƒè™‘å»ºç«‹ä¸€ä¸ªç®€å•çš„äºŒå‰å†³ç­–æ ‘ä»¥å®Œæˆä»»åŠ¡ï¼Œé‚£ä¹ˆå°±éœ€è¦æ‰¾åˆ°æ¯æ¬¡åˆ†å‰²çš„æœ€ä½³åˆ†å‰²ç‚¹ã€‚å¯»æ‰¾ä¾æ®ä¸ºä¿¡æ¯å¢ç›Šï¼Œå…¶å…¬å¼å¦‚ä¸‹

<div align = center>

$IG(Y,X) = H(Y)-\sum_{v\in Values(X)}P(v)Â·H(Y|X=v)$

</div>

$H(Y)$æ˜¯åˆ†å‰²å‰ç›®æ ‡å˜é‡$Y$çš„ç†µï¼Œ$Values(X)$ æ˜¯ç‰¹å¾$X$çš„æ‰€æœ‰å¯èƒ½å€¼.$P(v)$æ˜¯ç‰¹å¾$X$å–å€¼$v$çš„æ¦‚ç‡ã€‚$H(Yâˆ£X=v)$æ˜¯åœ¨ç‰¹å¾$X$å–å€¼$v$æ—¶ç›®æ ‡å˜é‡ $Y$çš„æ¡ä»¶ç†µã€‚
ç†µçš„å…¬å¼å¦‚ä¸‹ï¼š

<div align = center>

$H(Y)=-\sum_{i=1}^kp_i\log_2(p_i)$

</div>

ä¹‹åé€šè¿‡æ–¹æ³•é€’å½’å»ºç«‹å®Œæ•´çš„å†³ç­–æ ‘ã€‚

#### ä»£ç æ€è·¯åˆ†æ
æœ¬æ¬¡ä»»åŠ¡éœ€è¦åŸºäºå†³ç­–æ ‘æ„å»ºä¸€ä¸ªæœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œä½¿ç”¨Titanicæ•°æ®é›†é¢„æµ‹ä¹˜å®¢åœ¨æ²‰èˆ¹äº‹ä»¶ä¸­çš„ç”Ÿè¿˜æƒ…å†µã€‚å…·ä½“æ¥è¯´ï¼Œä»¥ä¹˜å®¢çš„æœ‰æ•ˆç‰¹å¾æ¥é¢„æµ‹ä»–ä»¬æ˜¯å¦ç”Ÿè¿˜ï¼Œé‚£ä¹ˆè¿™é‡Œå°±éœ€è¦å¯¹åŸå§‹æ•°æ®é›†è¿›è¡Œæ•°æ®å¡«å……å¹¶æ ¹æ®ç»éªŒç­‰ä¾æ®ç­›é€‰æ‰æ— æ•ˆçš„ç‰¹å¾ï¼Œä»¥æé«˜å†³ç­–æ ‘çš„æ•ˆæœã€‚ç”±äºå¹´é¾„ç‰¹å¾ä¸ºè¿ç»­æ•°æ®ï¼Œè€Œå†³ç­–æ ‘æ›´æ“…é•¿å¤„ç†ç¦»æ•£æ•°æ®ï¼Œå› æ­¤è¿™é‡Œè¿˜éœ€è¦å°†å¹´é¾„æ•°æ®åˆ†å—ï¼Œå°†å…¶æ„é€ ä¸ºç¦»æ•£æ•°æ®ã€‚å¹¶å°†å­—ç¬¦æ•°æ®è½¬æ¢ä¸ºæ•°å€¼æ•°æ®ä»¥ä¾¿äºæ¨¡å‹è®­ç»ƒã€‚
åœ¨å¤„ç†å®Œæ•°æ®ä¹‹åï¼Œå°±éœ€è¦è¿›è¡Œå†³ç­–æ ‘çš„æ„å»ºï¼Œå…·ä½“éœ€è¦çš„éƒ¨ä»¶ä¸Šæ–‡å·²ç»æè¿‡ï¼Œè¿™é‡Œä¸å†èµ˜è¿°ã€‚
æœ€åå°±æ˜¯æ¨¡å‹çš„è®­ç»ƒï¼Œè¯„ä¼°ä»¥åŠæµ‹è¯•ã€‚
### 4.ä»£ç å®ç°å’Œåˆ†æ
#### 4.1 æ•°æ®åŠ è½½å’Œé¢„å¤„ç†
å®šä¹‰`preprocess_data`æ–¹æ³•è¿›è¡Œæ•°æ®çš„è¯»å–å’Œé¢„å¤„ç†ï¼Œå…ˆé€šè¿‡`pandas`ä¸­çš„`read_csv`æ–¹æ³•è¯»å–å­˜å‚¨åœ¨csvæ–‡ä»¶ä¸­çš„æ•°æ®ï¼Œä¹‹åè¿›è¡Œé¢„å¤„ç†ï¼Œæ ¹æ®ç»éªŒåˆ é™¤ä¸€äº›æ— å…³çš„ç‰¹å¾ï¼Œç”¨ä¸­ä½æ•°å¡«å……ç¼ºå¤±å€¼ï¼Œå¹¶å°†å­—ç¬¦ç‰¹å¾ç¼–ç ä¸ºæ•°å€¼ç‰¹å¾ã€‚æœ€åå¯¹å¹´é¾„è¿›è¡Œåˆ†å—ï¼Œä½¿å…¶ä»è¿ç»­æ•°æ®å˜ä¸ºç¦»æ•£æ•°æ®ï¼Œä¾¿äºæ¨¡å‹è®­ç»ƒã€‚
åˆ©ç”¨`os`çš„ç›¸å…³æ–¹æ³•è·å–æ•°æ®å­˜å‚¨è·¯å¾„ï¼Œä½¿ç”¨å®šä¹‰çš„`preprocess_data`æ–¹æ³•å¯¹æ•°æ®è¿›è¡Œé¢„å¤„ç†ã€‚ä¹‹ååˆ†ç¦»æ•°æ®ç‰¹å¾å’Œæ ‡ç­¾ä¾¿äºåç»­è®­ç»ƒå’Œè¯„ä¼°
```python
import os
import pandas as pd
import numpy as np

# è¯»å–å’Œé¢„å¤„ç†æ•°æ®
def preprocess_data(file_path):
    data = pd.read_csv(file_path)

    # åˆ é™¤æ— å…³ç‰¹å¾
    data.drop(columns=["PassengerId", "Name", "Ticket", "Fare", "Cabin"], inplace=True)

    # å¡«å……ç¼ºå¤±å€¼
    data["Age"].fillna(data["Age"].mean(), inplace=True)
    data["Embarked"].fillna("S", inplace=True)

    # ç¼–ç åˆ†ç±»ç‰¹å¾
    data["Sex"] = data["Sex"].map({"male": 0, "female": 1})
    data["Embarked"] = data["Embarked"].map({"S": 0, "C": 1, "Q": 2})

    # åˆ†ç®±å¤„ç†è¿ç»­æ•°æ®Age
    # å¦‚æœå¹´é¾„å°äº1ï¼Œè®¾ä¸º0-1å²åŒºé—´ï¼›å…¶ä»–å¹´é¾„åˆ†æˆå¤šä¸ªåŒºé—´
    bins = [0, 1, 12, 20, 40, 60, 80]
    labels = [0, 1, 2, 3, 4, 5]
    data["Age"] = pd.cut(data["Age"], bins=bins, labels=labels, right=False)

    return data

# å®šä¹‰è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
train_file_path = os.path.join(current_dir, "train.csv")
test_file_path = os.path.join(current_dir, "test.csv")

# é¢„å¤„ç†æ•°æ®
train_data = preprocess_data(train_file_path)
test_data = preprocess_data(test_file_path)

# åˆ†ç¦»ç‰¹å¾å’Œæ ‡ç­¾
X_train = train_data.drop(columns=["Survived"])
y_train = train_data["Survived"]
```

#### 4.2 å†³ç­–æ ‘å®ç°
å®ç°ä¸€ä¸ªç®€å•çš„äºŒå‰å†³ç­–æ ‘ä»¥å®Œæˆè¿™æ¬¡ä»»åŠ¡ï¼Œ`fit`æ–¹æ³•ç”¨äºæ ‘çš„è®­ç»ƒï¼Œå…·ä½“çš„é€»è¾‘åœ¨`_grow_tree`æ–¹æ³•ä¸­ã€‚é¢„å®šä¹‰äº†æœ€å¤§æ·±åº¦å’Œæœ€å°æ ·æœ¬æ•°ä½œä¸ºå‰ªæå¤„ç†ï¼Œä½†ç”±äºåç»­è®­ç»ƒæ•ˆæœä¸é”™æ²¡æœ‰ç”¨åˆ°ã€‚ä¹‹åè°ƒç”¨`_best_split`æ–¹æ³•å¯»æ‰¾æœ€ä½³åˆ†å‰²ç‚¹ã€‚
åœ¨è¯¥æ–¹æ³•ä¸­ï¼Œé€šè¿‡è®¡ç®—ç‰¹å¾å€¼çš„æ‰€æœ‰é˜ˆå€¼å’Œå…¶å¯¹åº”çš„ç±»åˆ«æ¥æ‰¾å‡ºæœ€ä½³åˆ†å‰²ç‚¹ã€‚å…·ä½“é€»è¾‘ä¸ºå¯¹å…¶åˆ’åˆ†åå·¦å³å­æ ‘çš„æ•°é‡è¿›è¡Œéå†ï¼Œå¹¶ä¾æ¬¡é€šè¿‡`_information_gain`è®¡ç®—ä¿¡æ¯å¢ç›Šã€‚ä¿¡æ¯å¢ç›Šæœ€å¤§çš„åˆ’åˆ†æ–¹å¼å³ä¸ºæœ€ä½³åˆ†å‰²ç‚¹ã€‚
ä¹‹åæ ¹æ®æ‰¾åˆ°çš„æœ€ä½³åˆ†å‰²ç‚¹å¼€å§‹å­æ ‘çš„ç”Ÿé•¿ï¼Œè¿™é‡Œæ˜¯ä¸€ä¸ªé€’å½’çš„è¿‡ç¨‹ã€‚
`predict`æ–¹æ³•ç”¨äºè®­ç»ƒç»“æŸåçš„é¢„æµ‹ï¼Œé€»è¾‘ä¸ºæ ¹æ®æ•°æ®è¿›è¡Œæ ‘æŸ¥æ‰¾ï¼Œåˆ¤æ–­è¯¥æ•°æ®åº”è¯¥å±äºå“ªä¸€ä¸ªå¶èŠ‚ç‚¹å¹¶ç»™å‡ºé¢„æµ‹ç»“æœ
```python
# å†³ç­–æ ‘å®ç°
class SimpleDecisionTree:
    def __init__(self, max_depth=5):
        self.max_depth = max_depth
        self.tree = None

    def fit(self, X, y):
        self.tree = self._grow_tree(X, y, depth=0)

    def _grow_tree(self, X, y, depth):
        num_samples, num_features = X.shape
        # è¶…æ·±åº¦æˆ–è€…æ ·æœ¬æ•°å°äºç­‰äº1
        if depth >= self.max_depth or num_samples <= 1:
            return np.round(np.mean(y))  # å–å¹³å‡å€¼å¹¶å››èˆäº”å…¥

        # å¯»æ‰¾æœ€ä½³åˆ†å‰²ç‚¹
        best_feature, best_threshold = self._best_split(X, y, num_samples, num_features)
        if best_feature is None:
            return np.round(np.mean(y))

        indices_left = X[:, best_feature] < best_threshold
        left_subtree = self._grow_tree(X[indices_left], y[indices_left], depth + 1)
        right_subtree = self._grow_tree(X[~indices_left], y[~indices_left], depth + 1)
        return (best_feature, best_threshold, left_subtree, right_subtree)

    def _best_split(self, X, y, num_samples, num_features):
        best_gain = -1
        split_idx, split_threshold = None, None
        # è®¡ç®—ç‰¹å¾å€¼çš„æ‰€æœ‰å¯èƒ½é˜ˆå€¼åŠå…¶å¯¹åº”çš„ç±»åˆ«
        for feature_idx in range(num_features):
            thresholds, classes = zip(
                *sorted(zip(X[:, feature_idx], y))
            )  # å¯¹ç‰¹å¾å€¼å’Œç›®æ ‡å˜é‡æ’åº
            # åˆå§‹åŒ–å·¦å³å­æ ‘çš„æ ·æœ¬æ•°é‡
            num_left = [0] * 2
            num_right = [np.sum(y == 0), np.sum(y == 1)]
            for i in range(1, num_samples):
                c = classes[i - 1]
                num_left[c] += 1
                num_right[c] -= 1
                gain = self._information_gain(
                    y, classes, num_left, num_right
                )  # ä¿¡æ¯å¢ç›Š
                if thresholds[i] == thresholds[i - 1]:
                    continue
                if gain > best_gain:
                    best_gain = gain
                    split_idx = feature_idx
                    split_threshold = (thresholds[i] + thresholds[i - 1]) / 2
        return split_idx, split_threshold

    def _information_gain(self, y, classes, num_left, num_right):
        p = len(classes)
        p_left = sum(num_left) / p
        p_right = sum(num_right) / p

        if p_left == 0 or p_right == 0:
            return 0

        h = self._entropy(classes)
        h_left = self._entropy(classes[: sum(num_left)])
        h_right = self._entropy(classes[sum(num_left) :])

        return h - (p_left * h_left + p_right * h_right)

    def _entropy(self, y):
        hist = np.bincount(y)
        ps = hist / len(y)  # å„ç±»å‡ºç°æ¦‚ç‡
        return -np.sum([p * np.log2(p) for p in ps if p > 0])

    def predict(self, X):
        return [self._predict(inputs) for inputs in X]

    def _predict(self, inputs):
        node = self.tree
        while isinstance(node, tuple):
            if inputs[node[0]] < node[1]:
                node = node[2]
            else:
                node = node[3]
        return node

```
#### 4.3 æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°
è°ƒç”¨è‡ªå®šä¹‰çš„å†³ç­–æ ‘æ¨¡å‹ï¼Œå¹¶ä½¿ç”¨å¤„ç†å¥½çš„è®­ç»ƒæ•°æ®è¿›è¡Œè®­ç»ƒã€‚è®­ç»ƒå®Œæˆåç›´æ¥åœ¨è®­ç»ƒé›†ä¸Šè¿›è¡Œæ¨¡å‹è¯„ä¼°ã€‚

```python
# æ„å»ºæ¨¡å‹
model = SimpleDecisionTree()
model.fit(X_train.values, y_train.values)

# åœ¨è®­ç»ƒé›†ä¸Šè¯„ä¼°æ¨¡å‹
train_predictions = model.predict(X_train.values)
accuracy = np.mean(train_predictions == y_train.values)
print(f"Train Accuracy: {accuracy:.2f}")
```
#### 4.4 ä½œä¸šç»“æœå½•å…¥
å¯¹ä½œä¸šé›†æ•°æ®è¿›è¡Œé¢„æµ‹ï¼Œå°†é¢„æµ‹ç»“æœè½¬ä¸ºæ•´æ•°å½¢å¼å¹¶ä¿å­˜åˆ°åŸæ–‡ä»¶ä¸­
```python
# å¯¹æµ‹è¯•é›†è¿›è¡Œé¢„æµ‹
test_predictions = model.predict(test_data.values)

# å°†é¢„æµ‹ç»“æœè½¬æ¢ä¸ºæ•´æ•°å½¢å¼
test_predictions = [int(prediction) for prediction in test_predictions]

# ä¿å­˜ç»“æœåˆ°submission.csv
submission = pd.read_csv(test_file_path)[["PassengerId"]].copy()
submission["Survived"] = test_predictions
submission.to_csv(os.path.join(current_dir, "submission.csv"), index=False)
```
### 5.ç»“æœå±•ç¤º
![alt text](image-6.png)
æ®åŠ©æ•™åé¦ˆï¼Œæµ‹è¯•é›†æ­£ç¡®ç‡ä¸º0.774

---

## ä»»åŠ¡5ï¼šå²¸è¾¹åƒåœ¾è¯†åˆ«
### 1.ä»»åŠ¡æè¿°
æ°´ç¯å¢ƒä¿æŠ¤ä¸€ç›´å—åˆ°å¤§å®¶çš„é«˜åº¦å…³æ³¨ã€‚æ°´ä½“é™„è¿‘çš„åƒåœ¾ä¸ä»…ä»…ä¼šé€ æˆæ¶ˆæçš„è§†è§‰å†²å‡»ï¼Œè¿˜ä¼šç»§ç»­å¸¸å¸¸é€ æˆæ°´è´¨é—®é¢˜ï¼›æ‰€ä»¥éœ€è¦å®æ—¶ç›‘æµ‹æ°´ä½“é™„è¿‘çš„çš„ç”Ÿæ´»åƒåœ¾ç­‰ï¼Œä»¥ä¾¿åŠæ—¶å¤„ç†ã€‚é¡¹ç›®ç®—æ³•è¦è¾¾åˆ°çš„ç›®çš„ï¼šé€šè¿‡å¯¹å²¸è¾¹å’Œå²¸ä¸Šçš„åƒåœ¾è¿›è¡Œåˆ†å‰²ï¼Œå¹¶æŒ‰ç…§é¢ç§¯é˜ˆå€¼åˆ¤æ–­å¹¶è¾“å‡ºæŠ¥è­¦æ¶ˆæ¯
### 2.æ•°æ®é›†åˆ†æ
æ•°æ®é›†ä¸­åŒ…å«jpgå’Œpngä¸¤ç§æ ¼å¼çš„åŒåå›¾ç‰‡ï¼Œå…¶ä¸­jpgæ ¼å¼ä¸ºå›¾ç‰‡æ•°æ®ï¼Œpngæ ¼å¼ä¸ºå›¾ç‰‡çš„maskï¼Œmaskä¸­å¯¹æ°´ä½“å’Œåƒåœ¾ç­‰è¿›è¡Œåˆ†å‰²æ ‡æ³¨ï¼Œç±»åˆ«æœ‰ï¼šbackgroundï¼šèƒŒæ™¯(åƒç´ å€¼ï¼š0),algaeï¼šæ°´è—»(åƒç´ å€¼ï¼š1),dead_twigs_leavesï¼šæ¯æè´¥å¶(åƒç´ å€¼ï¼š2),garbageï¼šåƒåœ¾(åƒç´ å€¼ï¼š3),waterï¼šæ°´ä½“(åƒç´ å€¼ï¼š4)ã€‚
### 3.ä»»åŠ¡åˆ†æ
æœ¬æ¬¡ä»»åŠ¡å±äºè¯­ä¹‰åˆ†å‰²ä»»åŠ¡ï¼Œè¦æ±‚å¯¹å²¸è¾¹å’Œå²¸ä¸Šçš„åƒåœ¾è¿›è¡Œåˆ†å‰²ï¼Œå¹¶æ ¹æ®é¢ç§¯é˜ˆå€¼åˆ¤æ–­å¹¶è¾“å‡ºæŠ¥è­¦ä¿¡æ¯ã€‚ç”±äºä»»åŠ¡æ¯”è¾ƒå¤æ‚ï¼Œä¸ªäººè®¤ä¸ºè‡ªè¡Œç¼–å†™çš„æ¨¡å‹å¯èƒ½ä¸èƒ½èƒœä»»ï¼Œå› æ­¤å‡†å¤‡é‡‡ç”¨é¢„è®­ç»ƒçš„`deeplabv3_resnet101`æ¨¡å‹ã€‚
#### deeplabv3
è¯¥æ¨¡å‹æ˜¯Googleæå‡ºçš„ç”¨äºè¯­ä¹‰åˆ†å‰²çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚å®ƒé€šè¿‡å¼•å…¥å¤šç§æŠ€æœ¯ï¼Œæ˜¾è‘—æé«˜äº†åˆ†å‰²ç²¾åº¦å’Œæ•ˆç‡ï¼Œå°¤å…¶åœ¨å¤æ‚åœºæ™¯å’Œå¤šå°ºåº¦å¯¹è±¡è¯†åˆ«ä¸­è¡¨ç°å‡ºè‰²
deeplabv3çš„æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡ç©ºæ´å·ç§¯å’Œç©ºé—´é‡‘å­—å¡”æ± åŒ–æ¥æé«˜æ„Ÿå—é‡å’Œæ•è·å¤šå°ºåº¦ä¿¡æ¯ã€‚åœ¨å…¶æ¨¡å‹æ¶æ„ä¸­ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ éƒ¨åˆ†ï¼š
+ ä¸»å¹²ç½‘ç»œ
    ä½¿ç”¨é¢„è®­ç»ƒçš„å·ç§¯ç¥ç»ç½‘ç»œä½œä¸ºç‰¹å¾æå–å™¨ï¼Œä½œç”¨ä¸ºæå–é«˜è´¨é‡çš„ç‰¹å¾å›¾ä»¥ä¾¿åç»­å¤„ç†
+ ASPPæ¨¡å—
    åŒ…å«å¤šä¸ªä¸åŒç©ºæ´ç‡çš„å·ç§¯å±‚ï¼Œç”¨äºæ•è·ä¸åŒå°ºåº¦çš„ç‰¹å¾ã€‚å¹¶ç»“åˆå…¨å‰§å¹³å‡æ± åŒ–æ¥è¿›ä¸€æ­¥å¢å¼ºå…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚æœ€åå°†ä¸åŒå°ºåº¦çš„ç‰¹å¾æ‹¼æ¥åœ¨ä¸€èµ·ï¼Œå½¢æˆä¸€ä¸ªä¸°å¯Œçš„ç‰¹å¾è¡¨ç¤º
ç¡®å®šä½¿ç”¨çš„é¢„è®­ç»ƒæ¨¡å‹åï¼Œå¼€å§‹æ„æ€ä»£ç æ¡†æ¶ã€‚ä¸»ä½“æ¡†æ¶ä¸å…¶ä»–æœºå™¨å­¦ä¹ ä»»åŠ¡ç±»ä¼¼ï¼Œä¹Ÿå°±æ˜¯æ•°æ®è¯»å–å’Œé¢„å¤„ç†ï¼Œæ¨¡å‹è®­ç»ƒï¼Œæ¨¡å‹è¯„ä¼°å’Œæµ‹è¯•ã€‚ä½†ç”±äºè¿™æ¬¡ä»»åŠ¡çš„å¤æ‚æ€§ï¼Œå°†ä¼šç”¨åˆ°å¤šç§æ‰‹æ®µä»¥å¢å¼ºè®­ç»ƒæ•ˆæœã€‚
åŒæ—¶ç”±äºæœ¬æ¬¡é¡¹ç›®åœ¨å¹³å°ä¸Šå®Œæˆï¼Œè¿˜éœ€è¦ç¼–å†™ä¸€ä¸ª`ji.py`ä»¥å®Œæˆæ¥å£

### 4.ä»£ç å®ç°å’Œåˆ†æ

#### 4.1å…¨å±€è®¾ç½®
ä¸çŸ¥é“ä¸ºä»€ä¹ˆï¼Œæˆ‘çš„`print`æ–¹æ³•å¹¶ä¸èƒ½åœ¨å¹³å°çš„å®æ—¶æ—¥å¿—ä¸­æ­£å¸¸è¾“å‡ºï¼Œå› æ­¤æ·»åŠ ä»¥ä¸‹è®¾ç½®ä»¥ä¿è¯ç¨‹åºè¿è¡Œä¿¡æ¯çš„æœ‰æ•ˆè¾“å‡ºã€‚ä½¿ç”¨`logging`å®ç°è¾“å‡ºï¼Œå¹¶é€šè¿‡`os`ç¦ç”¨è¾“å‡ºç¼“å­˜ï¼Œå¹¶é‡å®šå‘åˆ°æ ‡å‡†è¾“å‡ºæµã€‚è™½ä¸æ˜¯å¾ˆæ˜ç™½èƒŒååŸç†ï¼Œä½†æ˜¯è¿™æ ·ç¡®å®ä¿è¯äº†è¿è¡Œä¿¡æ¯çš„æ­£å¸¸è¾“å‡ºã€‚åŒæ—¶è¿™ä¸€éƒ¨åˆ†è¿˜åŒ…æ‹¬äº†æ•´ä¸ªç¨‹åºç”¨åˆ°çš„æ‰€æœ‰ç¬¬ä¸‰æ–¹åº“

```python
import os
import sys
import torch
import numpy as np
import torch.nn as nn
import torch.optim as optim
from torchvision import models
import logging
import random
from torch.utils.tensorboard import SummaryWriter
from torch.optim.lr_scheduler import ReduceLROnPlateau
from torchmetrics import JaccardIndex
from tqdm import tqdm
from torch.cuda.amp import GradScaler,autocast
from PIL import Image
from torch.utils.data import Dataset, DataLoader
import shutil
import matplotlib.pyplot as plt
import albumentations as A
from albumentations.pytorch import ToTensorV2
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
sys.stdout = os.fdopen(sys.stdout.fileno(), 'w', buffering=1)
os.environ['PYTHONUNBUFFERED'] = '1'

```
#### 4.2 æ•°æ®è¯»å–å’Œé¢„å¤„ç†
å®šä¹‰`SegmentationDataset`æ•°æ®é›†ç±»ä»¥è¿›è¡Œæ•°æ®çš„è¯»å–ï¼Œå…¶ä¸­ç”±äºå›¾ç‰‡æ•°æ®å’Œå¯¹åº”maskåªæœ‰åç¼€ä¸åŒï¼Œå› æ­¤è®¾å®špathæ—¶å°†å›¾ç‰‡è·¯å¾„çš„åç¼€è½¬ä¸ºpngå°±è·å¾—äº†maskçš„è·¯å¾„ã€‚
æ•°æ®é¢„å¤„ç†åˆ™æ˜¯å®šä¹‰äº†`data_process`æ–¹æ³•ä»¥åŠè¿›è¡Œæ•°æ®é¢„å¤„ç†ã€‚å…ˆè¯»å–äº†æ‰€æœ‰æ•°æ®çš„ç›®å½•ï¼Œç”±äºæœ‰ä¸¤ä¸ªæ•°æ®é›†ï¼Œå› æ­¤åˆ›å»ºä¸€ä¸ª`merged`ç›®å½•å…ˆå°†ä¸¤ä¸ªæ•°æ®é›†åˆå¹¶ï¼Œå†åˆ›å»º`train`å’Œ`val`ç›®å½•ç”¨äºè®­ç»ƒé›†éªŒè¯é›†çš„åˆ’åˆ†ã€‚
```python
class SegmentationDataset(Dataset):
    def __init__(self, data_dir, transform=None):

        self.data_dir = data_dir
        self.image_names = [f for f in os.listdir(data_dir) if f.endswith(".jpg")]
        self.transform = transform

    def __len__(self):
        return len(self.image_names)

    def __getitem__(self, idx):
        img_name = self.image_names[idx]
        img_path = os.path.join(self.data_dir, img_name)
        mask_path = os.path.join(self.data_dir, img_name.replace(".jpg", ".png"))

        image = Image.open(img_path)
        image = image.convert("RGB")
        mask = Image.open(mask_path).convert("L")

        image= np.array(image)
        mask = np.array(mask)

        if self.transform:
            augmented = self.transform(image=image, mask=mask)
            image = augmented["image"]
            mask = augmented["mask"]

        mask = torch.as_tensor(mask, dtype=torch.long)
        return image, mask

def data_process(dir_path):
    data_path_1 = os.path.join(dir_path, "1704")
    data_path_2 = os.path.join(dir_path, "3288")
    merged_path = os.path.join(dir_path, "merged")
    train_path = os.path.join(dir_path, "train")
    val_path = os.path.join(dir_path, "val")

    # åˆ›å»ºæ–‡ä»¶å¤¹
    if not os.path.exists(merged_path):
        os.makedirs(merged_path)
    if not os.path.exists(train_path):
        os.makedirs(train_path)
    if not os.path.exists(val_path):
        os.makedirs(val_path)

    # åˆå¹¶æ•°æ®
    for data_path in [data_path_1, data_path_2]:
        for file_name in os.listdir(data_path):
            file_path = os.path.join(data_path, file_name)
            if file_name.endswith(".jpg") or file_name.endswith(".png"):
                dest_path = os.path.join(merged_path, file_name)
                try:
                    shutil.copy(file_path, dest_path)
                except Exception:
                    logger.info(f"Failed to copy file: {file_path}")

                # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æˆåŠŸå¤åˆ¶ï¼Œå¦‚æœæ²¡æœ‰åˆ™é‡è¯•
                if not os.path.exists(dest_path):
                    logger.info(f"Retry copying file: {file_path}")
                    try:
                        shutil.copy(file_path, dest_path)
                    except Exception:
                        logger.info(f"Failed to copy file on retry: {file_path}")

    # è·å–mergedæ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰æ–‡ä»¶å
    file_names = [f for f in os.listdir(merged_path) if f.endswith(".jpg")]

    # è®¡ç®—åˆ’åˆ†æ¯”ä¾‹
    train_ratio = 0.9
    num_train = int(len(file_names) * train_ratio)
    num_val = len(file_names) - num_train

    logger.info(f"Total files: {len(file_names)}, Train: {num_train}, Val: {num_val}")

    # éšæœºæ‰“ä¹±æ–‡ä»¶åé¡ºåº
    random.shuffle(file_names)

    # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
    for i, file_name in enumerate(file_names):
        img_path = os.path.join(merged_path, file_name)
        mask_name = file_name.replace(".jpg", ".png")
        mask_path = os.path.join(merged_path, mask_name)

        if os.path.exists(img_path) and os.path.exists(mask_path):
            if i < num_train:
                shutil.move(img_path, os.path.join(train_path, file_name))
                shutil.move(mask_path, os.path.join(train_path, mask_name))
            else:
                shutil.move(img_path, os.path.join(val_path, file_name))
                shutil.move(mask_path, os.path.join(val_path, mask_name))
        else:
            logger.info(f"File pair missing: {img_path}, {mask_path}")

```
æ•°æ®å¢å¼ºç”¨åˆ°çš„transformå¦‚ä¸‹:
å¯¹è®­ç»ƒé›†å’ŒéªŒè¯é›†åšäº†ä¸åŒçš„å¢å¼ºï¼Œå› ä¸ºéªŒè¯é›†å¹¶ä¸éœ€è¦ç¿»è½¬éšæœºé‡‡æ ·ç­‰æ“ä½œä»¥å¢åŠ æ•°æ®é‡ï¼Œåªéœ€è¦ä¿è¯æ•°æ®å°ºå¯¸ç›¸åŒï¼Œè¿›è¡Œç›¸åŒæ ‡å‡†åŒ–å³å¯ã€‚
```python
    train_transform = A.Compose([
        A.HorizontalFlip(p=0.5),
        A.VerticalFlip(p=0.5),
        A.RandomRotate90(p=0.5),
        A.Rotate(limit=15, p=0.5),
        A.Resize(256, 256),
        A.RandomCrop(224, 224),
        A.GaussianBlur(p=0.5),  # æ·»åŠ é«˜æ–¯æ¨¡ç³Š
        A.RandomBrightnessContrast(p=0.5),  # æ·»åŠ éšæœºäº®åº¦å¯¹æ¯”åº¦è°ƒæ•´
        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), always_apply=True, p=1.0),
        ToTensorV2()
    ], additional_targets={'mask': 'mask'})
    
    val_transform = A.Compose([
        A.Resize(256, 256),
        A.CenterCrop(224, 224),
        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), always_apply=True, p=1.0),
        ToTensorV2()
    ], additional_targets={'mask': 'mask'})
    
```

#### 4.3æ¨¡å‹è®­ç»ƒ
å®šä¹‰`train`æ–¹æ³•ä»¥å®ç°æ¨¡å‹è®­ç»ƒã€‚ä½¿ç”¨`tensorboard`æ¥è®°å½•è®­ç»ƒä¸­çš„å‚æ•°å˜åŒ–ä»¥åŠç»˜åˆ¶å›¾è¡¨ã€‚ç”¨`tqdm`å®ç°è¿›åº¦æ¡ä»¥æŒæ¡æ¯ä¸ªepochçš„è®­ç»ƒè¿›åº¦ã€‚åœ¨æ¯ä¸ªepochçš„è®­ç»ƒç»“æŸåéƒ½è¿›è¡Œä¸€æ¬¡éªŒè¯ï¼Œä»¥éªŒè¯é›†çš„lossä¸ºæ ‡å‡†è¿›è¡Œå­¦ä¹ ç‡çš„æ›´æ–°ï¼ŒåŒæ—¶æ¯”è¾ƒè¯¥epochè®­ç»ƒçš„æ¨¡å‹æ˜¯å¦ä¸ºæ•ˆæœæœ€å¥½çš„ï¼Œå¦‚æ˜¯åˆ™ä¿å­˜ã€‚æ¯5ä¸ªepoché¢å¤–ä¿å­˜ä¸€æ¬¡æ¨¡å‹ï¼Œæ–¹ä¾¿è®­ç»ƒä¸­æ–­æ¢å¤ã€‚
é€šè¿‡`evaluate_model`æ–¹æ³•è¿›è¡ŒéªŒè¯ä¸losså’Œmiouçš„è®¡ç®—ã€‚miouè®¡ç®—å€ŸåŠ©äº†`JaccardIndex`ç±»ã€‚
```python
def train(
    model,
    train_loader,
    test_loader,
    criterion,
    optimizer,
    scheduler,
    best_model_path,
    model_path,
    log_dir,
    num_epochs=5,
):
    # tensorboardè®°å½•å™¨
    writer = SummaryWriter(log_dir=log_dir)
    scaler = GradScaler()
    best_val_loss = float("inf")
    best_epoch = -1
    best_val_miou = 0

    # æ¨¡å‹è®­ç»ƒ
    logger.info("begin train")
    for epoch in range(num_epochs):
        running_loss = 0.0
        model.train()
        for images, masks in tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs}",file=sys.stdout):
            images = images.to(device)
            masks = masks.to(device)

            optimizer.zero_grad()
            with autocast():
                outputs = model(images)["out"]
                loss = criterion(outputs, masks)

            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()

            running_loss += loss.item()

        avg_loss = running_loss / len(train_loader)
        current_lr = optimizer.param_groups[0]["lr"]  # è·å–å½“å‰çš„å­¦ä¹ ç‡

        logger.info(
            f"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss}, lr: {current_lr}"
        )
        writer.add_scalar("train_loss", avg_loss, epoch)
        writer.add_scalar("lr", current_lr, epoch)  # è®°å½•å­¦ä¹ ç‡

        # éªŒè¯
        val_loss, val_miou = evaluate_model(model, test_loader, criterion)
        logger.info(f"Val Loss [{epoch+1}/{num_epochs}]: {val_loss}, mIoU: {val_miou}")
        writer.add_scalar("val_loss", val_loss, epoch)
        writer.add_scalar("miou", val_miou, epoch)

        # æ›´æ–°æœ€å¥½çš„æ¨¡å‹
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            best_val_miou = val_miou
            best_epoch = epoch + 1
            torch.save(model.state_dict(), best_model_path)
            logger.info(
                f"best val_loss's epoch={best_epoch}, best_val_Loss={best_val_loss}, best_mIoU={best_val_miou}"
            )

        # æ›´æ–°å­¦ä¹ ç‡
        scheduler.step(val_loss)
        if epoch%5==0:
            torch.save(model.state_dict(), model_path)
            logger.info(f"Epoch{epoch},save the models")
    
    writer.close()
    torch.save(model.state_dict(), model_path)
    logger.info("æ¨¡å‹å·²ä¿å­˜")
    logger.info(
        f"best val_loss={best_val_loss}, best val_miou={best_val_miou}, bset epoch={best_epoch}"
    )
def evaluate_model(model, dataloader, criterion):
    model.eval()
    logger.info(f"val dataset's length is {len(dataloader)}")
    total_loss = 0
    total_miou = 0
    miou_metric = JaccardIndex(task="multiclass",num_classes=5, ignore_index=None).to(device)  # å®šä¹‰mIoUæŒ‡æ ‡

    with torch.no_grad():
        for images, masks in tqdm(dataloader, desc="Validation", leave=False,file=sys.stdout):
            images = images.to(device)
            masks = masks.to(device)
            outputs = model(images)["out"]
            loss = criterion(outputs, masks)
            total_loss += loss.item()

            # è®¡ç®—miou
            preds = torch.argmax(outputs, dim=1)
            miou = miou_metric(preds, masks)
            total_miou += miou.item()
    avg_loss = total_loss / len(dataloader)
    avg_miou = total_miou / len(dataloader)

    return avg_loss, avg_miou
```
#### 4.4 æ¨¡å‹æµ‹è¯•å’Œç»“æœå¯è§†åŒ–
é€šè¿‡`visualize_prediction`æ–¹æ³•è¿›è¡Œæ¨¡å‹æµ‹è¯•å’Œç»“æœçš„å¯è§†åŒ–ã€‚å…ˆè°ƒç”¨è®­ç»ƒå¥½çš„æ¨¡å‹å¯¹æµ‹è¯•é›†æ•°æ®è¿›è¡Œé¢„æµ‹ï¼Œå¹¶å¯¹é¢„æµ‹ç»“æœè¿›è¡Œé€†å½’ä¸€åŒ–ä»¥æ–¹ä¾¿åç»­å¯è§†åŒ–ã€‚å¯è§†åŒ–æ—¶é€šè¿‡`decode_segmap`æ–¹æ³•è¿›è¡Œé¢œè‰²æ˜ å°„ï¼Œå°†maskæ›´åŠ ç›´è§‚çš„å±•ç°å‡ºæ¥ã€‚
```python
def visualize_prediction(model, test_loader, device, epoch):
    model.eval()
    with torch.no_grad():
        for images, masks in test_loader:
            images, masks = images.to(device), masks.to(device)
            outputs = model(images)['out']
            predictions = torch.argmax(outputs, dim=1)

            images = images.cpu().numpy()
            predictions = predictions.cpu().numpy()
            masks = masks.cpu().numpy()

            # é€†å½’ä¸€åŒ–æ“ä½œ
            mean = np.array([0.485, 0.456, 0.406])
            std = np.array([0.229, 0.224, 0.225])
            images = (images * std[None, :, None, None]) + mean[None, :, None, None]
            images = np.clip(images, 0, 1)  # ç¡®ä¿èŒƒå›´åœ¨ [0, 1]

            # ç¡®ä¿æœ‰è‡³å°‘3ä¸ªæ ·æœ¬è¿›è¡Œå¯è§†åŒ–
            num_samples = min(images.shape[0], 3)
            for i in range(num_samples):
                fig, ax = plt.subplots(1, 3, figsize=(18, 6))

                ax[0].imshow(images[i].transpose(1, 2, 0))  # åŸå§‹å›¾ç‰‡
                ax[0].set_title("Original Image")
                ax[0].axis("off")

                ax[1].imshow(decode_segmap(predictions[i]))  # é¢„æµ‹çš„mask
                ax[1].set_title("Predicted Mask")
                ax[1].axis("off")

                ax[2].imshow(decode_segmap(masks[i]))  # çœŸå®çš„mask
                ax[2].set_title("Ground Truth Mask")
                ax[2].axis("off")

                plt.savefig(
                    f"/project/train/result-graphs/prediction-epoch{i}.png"
                )
                plt.close(fig)  # ç¡®ä¿æ¯æ¬¡ç»˜åˆ¶åå…³é—­å›¾å½¢

def decode_segmap(mask):
    r = np.zeros_like(mask).astype(np.uint8)
    g = np.zeros_like(mask).astype(np.uint8)
    b = np.zeros_like(mask).astype(np.uint8)

    label_colors = np.array(
        [
            (0, 0, 0),         # èƒŒæ™¯
            (128, 0, 0),       # æ°´è—»
            (0, 128, 0),       # æ¯æè´¥å¶
            (128, 128, 0),     # åƒåœ¾
            (0, 0, 128)    # æ°´ä½“
        ]
    )

    for label in range(0, 5):
        r[mask == label] = label_colors[label, 0]
        g[mask == label] = label_colors[label, 1]
        b[mask == label] = label_colors[label, 2]

    rgb = np.stack([r, g, b], axis=2)
    return rgb / 255.0  # ä¿æŒå½’ä¸€åŒ–åˆ° [0, 1]
```
#### 4.5 ç¨‹åºæ€»é€»è¾‘
åœ¨`main` å‡½æ•°ä¸­è¿›è¡Œäº†ç¨‹åºæ‰€æœ‰åŠŸèƒ½æ¨¡å—çš„ä¸²è”ã€‚
```python
def main():
    batch_size = 32  # æ ¹æ®æ˜¾å­˜å¤§å°è°ƒæ•´
    num_workers = 8  # æ ¹æ® CPU æ ¸å¿ƒæ•°è°ƒæ•´
    num_epochs = 100 # è®­ç»ƒè½®æ•°

    # æ•°æ®å¢å¼º
    train_transform = A.Compose([
        A.HorizontalFlip(p=0.5),
        A.VerticalFlip(p=0.5),
        A.RandomRotate90(p=0.5),
        A.Rotate(limit=15, p=0.5),
        A.Resize(256, 256),
        A.RandomCrop(224, 224),
        A.GaussianBlur(p=0.5),  # æ·»åŠ é«˜æ–¯æ¨¡ç³Š
        A.RandomBrightnessContrast(p=0.5),  # æ·»åŠ éšæœºäº®åº¦å¯¹æ¯”åº¦è°ƒæ•´
        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), always_apply=True, p=1.0),
        ToTensorV2()
    ], additional_targets={'mask': 'mask'})
    
    val_transform = A.Compose([
        A.Resize(256, 256),
        A.CenterCrop(224, 224),
        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), always_apply=True, p=1.0),
        ToTensorV2()
    ], additional_targets={'mask': 'mask'})
    

    # è®¾ç½®è·¯å¾„
    data_dir = "/home/data/"
    train_path = os.path.join(data_dir, "train")
    val_path = os.path.join(data_dir, "val")
    model_path = "/project/train/models/deeplabv3.pth"
    log_dir = "/project/train/tensorboard/"
    best_model_path = "/project/train/models/deeplabv3_best.pth"

    # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
    model = models.segmentation.deeplabv3_resnet101(pretrained=True)
    model.classifier[4] = nn.Conv2d(256, 5, kernel_size=1)  # é€šé“æ•°ä¿®æ”¹ä¸º5ï¼Œå¯¹åº”ç±»åˆ«æ•°
    # åŠ è½½è®­ç»ƒæœ€å¥½çš„æ¨¡å‹
    if os.path.exists(best_model_path):
        model.load_state_dict(torch.load(best_model_path))
    model.to(device)

    # æ•°æ®é¢„å¤„ç†
    data_process(dir_path=data_dir)

    # æ•°æ®é›†åŠ è½½
    train_dataset = SegmentationDataset(train_path, train_transform)
    test_dataset = SegmentationDataset(val_path,val_transform)  # testé€šå¸¸ä¸åšæ•°æ®å¢å¼º
    train_loader = DataLoader(
        train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers
    )
    test_loader = DataLoader(
        test_dataset, batch_size=8, shuffle=True, num_workers=num_workers
    )

    # æŸå¤±å‡½æ•°ï¼Œä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)
    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True,min_lr=1e-8)

    train(
        model,
        train_loader,
        test_loader,
        criterion,
        optimizer,
        scheduler,
        best_model_path,
        model_path,
        log_dir,
        num_epochs,
    )

    model.load_state_dict(torch.load(best_model_path))
    
    # è°ƒç”¨é¢„æµ‹å’Œå¯è§†åŒ–å‡½æ•°
    visualize_prediction(model, test_loader, device,num_epochs+1)

```

#### 4.6 æ¥å£å®šä¹‰
åœ¨`ji.py`ä¸­è¿›è¡Œäº†å¹³å°éœ€è¦çš„æ¥å£å®šä¹‰ï¼Œå…·ä½“å¦‚ä¸‹ï¼š
ä¸»ä½“ä¸ºå¹³å°è§„å®šæ ¼å¼ã€‚
```python
# ji.py
import json
import torch
import cv2
from PIL import Image
import numpy as np
from torchvision import models, transforms
import albumentations as A
from albumentations.pytorch import ToTensorV2

# åˆå§‹åŒ–æ¨¡å‹
def init():
    model = torch.load("ev_sdk/src/model_1599.pkl",map_location=torch.device('cuda'))
    model.classifier[4] = torch.nn.Conv2d(256, 5, kernel_size=1)  # é€šé“æ•°ä¿®æ”¹ä¸º5ï¼Œå¯¹åº”ç±»åˆ«æ•°
    model.load_state_dict(torch.load('/project/train/models/deeplabv3.pth',map_location=torch.device('cuda')))
    model.to(torch.device('cuda'))
    model.eval()
    return model

# å¤„ç†å›¾åƒ
def process_image(handle=None, input_image=None, args=None, **kwargs):
    args = json.loads(args)
    mask_output_path = args.get('mask_output_path', '')

    # å›¾åƒè½¬æ¢
    image_transform = A.Compose([
        A.Resize(256, 256),
        A.CenterCrop(224, 224),
        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), always_apply=True, p=1.0),
        ToTensorV2()
    ])

    # å°† BGR è½¬æ¢ä¸º RGB å¹¶åº”ç”¨å›¾åƒè½¬æ¢
    input_image = cv2.cvtColor(input_image, cv2.COLOR_BGR2RGB)
    image_array = np.array(input_image)
    input_tensor = image_transform(image=image_array)['image'].unsqueeze(0).to(torch.device('cuda'))

    # ç”Ÿæˆæ©ç 
    with torch.no_grad():
        output = handle(input_tensor)['out'][0]
        
    output_predictions = torch.argmax(output, dim=0)
    output_predictions = output_predictions.cpu().numpy().astype(np.uint8)
    
    height, width = input_image.shape[:2]

    transforms_mask=A.Compose([
        A.Resize(height,width),
    ])
    output_predictions= transforms_mask(image=output_predictions)['image']

    if input_image.shape[:2] != output_predictions.shape:
        raise ValueError(f"Mismatch in dimensions: Input image shape {input_image.shape[:2]}, Output predictions shape {output_predictions.shape}")
    # ä¿å­˜æ©ç 
    if mask_output_path:
        output_image = Image.fromarray(output_predictions)
        output_image.save(mask_output_path)

    # è§£æè¾“å‡ºï¼ŒæŸ¥æ‰¾ç›®æ ‡å¯¹è±¡
    
    objects = []
    target_info = []
    is_alert = False

    # æŸ¥æ‰¾åƒåœ¾å¯¹è±¡
    garbage_mask = (output_predictions == 3).astype(np.uint8)
    num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(garbage_mask, connectivity=8)

    for i in range(1, num_labels):
        x, y, w, h, area = stats[i]
        area_ratio = area / (width * height)

        if area_ratio > 0.02:  # é˜ˆå€¼ä¸º0.02
            target = {
                "x": int(x),
                "y": int(y),
                "height": int(h),
                "width": int(w),
                "name": "garbage",
                "area_ratio": float(area_ratio)
            }
            objects.append(target)
            target_info.append(target)
            is_alert = True

    result = {
        "algorithm_data": {
            "is_alert": is_alert,
            "target_count": len(target_info),
            "target_info": target_info
        },
        "model_data": {
            "objects": objects
        }
    }

    if mask_output_path:
        result["model_data"]["mask"] = mask_output_path

    return json.dumps(result, indent=4)
```

### 5.ç»“æœå±•ç¤º
#### æœ€ç»ˆæ¨¡å‹çš„ç»“æœå¯è§†åŒ–
![alt text](<FNCBYSHNV(@[KV9D4$PGGLA.png>)  
#### è®­ç»ƒè®°å½•ï¼š
![alt text](è®­ç»ƒ.png)  
#### æ‰“æ¦œè®°å½•ï¼š
![alt text](æˆ‘çš„æ‰“æ¦œ.png)  
#### æ‰“æ¦œæ’å
![alt text](æ¦œå•æ’å.jpg)  
#### æ¨¡å‹æµ‹è¯•è®°å½•ï¼š
![alt text](æ¨¡å‹æµ‹è¯•.png)  
#### æµ‹è¯•ä»»åŠ¡
![alt text](æµ‹è¯•ä»»åŠ¡ä¿¡æ¯ï¼ˆä¸ŠåŠï¼‰.png)  
![alt text](æµ‹è¯•ä»»åŠ¡ä¿¡æ¯ï¼ˆä¸‹åŠï¼‰.png)  